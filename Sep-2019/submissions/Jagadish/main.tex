% link to instruction: https://tc.computer.org/tcde/tcde-bulletin-author-instructions/
\documentclass[11pt]{article} 
% \usepackage[utf8]{inputenc}

\usepackage{deauthor,times}
\usepackage{graphicx} % 
\usepackage{hyperref}
\usepackage{wrapfig}
% \usepackage{comment}
% \graphicspath{{asudeh/}}
\usepackage{amsfonts,amsmath}



% Use \begin{document} immediately before the title and authors are given.
\begin{document}

% \title{Responsible Scoring and Ranking by Human-designed and Machine-learned Algorithms}
\title{Towards Responsible Data-driven Decision Making in Score-Based Systems
\footnote{This work was supported in part by NSF Grants No. 1741022 and 1926250.}
}
\author{Abolfazl Asudeh\footnotemark[2], H. V. Jagadish\footnotemark[3], Julia Stoyanovich\footnotemark[4] \\
\footnotemark[2]  University of Illinois at Chicago, \footnotemark[3] University of Michigan, \footnotemark[4] New York University\\
\footnotemark[2] {asudeh@uic.edu}, \footnotemark[3] {jag@umich.edu}, \footnotemark[4] {stoyanovich@nyu.edu}
}
% \date{}

\maketitle


\begin{abstract}
Human decision makers often receive assistance from data-driven algorithmic systems that provide a {\em score} for evaluating the quality of items such as products, services, or individuals. These scores can be obtained by combining different features either through a process learned by ML models, or using a weight vector designed by human experts, with their past experience and notions of what constitutes item quality. The scores can be used for different evaluation purposes such as ranking or classification.

% may be derived in different ways.  For example, they may be learned from training data, or computed as a linear combination of attribute values, using a weight vector specified by a human expert.   We refer to the entities that determine how an item is to be scored as {\em evaluators}.  An evaluator may be a human expert, with her past experience and notions of what constitutes item quality, or it may be a machine that learns how to score items based on training data.  

%can be obtained by combining different features through a complex black-box process learned by ML models,  or by linearly combining some decision attributes, using a weight vector designed by human experts or machine-learned using some training data.  

%In this paper we view decision making in score-based systems through the lens of responsibility.
%
%While using these systems 
%can help make society safer, more just, and more prosperous, irresponsible implementation of these technologies may not only fail to help, but even harm us on an unprecedented scale.
%

In this paper, we view the design of these scores through the lens of responsibility.  %We consider human experts (for human-designed evaluators) and training data (for machine-learned evaluators) as key factors to achieve responsible data-driven decision making,
%Viewing the experts and training data as the key factors for human-designed and machine-learned scores, 
We present technical methods (i) to assist human experts in designing fair and stable score-based rankings and (ii) to assess and (if needed) enhance the coverage of a training dataset for machine learning tasks such as classification.
% present technical methods for assisting (human) experts in designing scoring methods that lead to fair and stable rankings.  We also present algorithms that help machine evaluators assess  the coverage of a training dataset for a task such as classification, and mitigate the lack of coverage when necessary.
 \end{abstract}
\input{submissions/Jagadish/intro.tex}
\input{submissions/Jagadish/Ranking.tex}
\input{submissions/Jagadish/coverage}
\vspace{-3mm}\section{Final Remarks}\label{sec:conclusion}
In this article we explained our results towards responsible data-driven decision making in score-based systems.
The scores, in these systems, are obtained by combining some features using either machine learning models or human-designed weight vectors.
We provided our results for (i) assisting the experts to design fair and stable rankings, and (ii) assessing and enhancing coverage in a (given) training dataset for tasks such as classification.

So far, in (i) our focus has been on ranking, where the scores are used for comparing the items in a pool. Human-designed scores are also used for tasks such as classification.
Extending our results for these tasks is part of our future work.
Also, we would like to adopt the proposed techniques for linear machine learning models.
The idea is to first train a machine learning model and then adjust the model to, for example, satisfy some fairness criteria. A similar idea can also be applied for designing ensemble methods for combining the outcome of multiple ML models.
In (ii), we used a fixed threshold across different value combinations, representing ``minor subgroups''.
We consider further investigations on identifying threshold value and minor subgroups for future work.
We will also investigate other properties (in addition to coverage) for assessing and enhancing the fitness of training data for responsible data science tasks.




% \bibliographystyle{unsrt}
% \small
% \bibliography{ref}

\begin{thebibliography}{10}
\itemsep=1pt
\begin{small}
\vspace{-3mm}

\bibitem{marcus2011human}
A.~Marcus, E.~Wu, D.~Karger, S.~Madden, and R.~Miller.
\newblock Human-powered sorts and joins.
\newblock {\em PVLDB}, 5(1):13--24, 2011.

\bibitem{asudeh2015pareto}
A.~Asudeh, G.~Zhang, N.~Hassan, C.~Li, and G.~V. Zaruba.
\newblock Crowdsourcing pareto-optimal object finding by pairwise comparisons.
\newblock In {\em CIKM}, pages 753--762. ACM, 2015.

\bibitem{fifa}
FIFA.
\newblock Fifa/coca-cola world ranking procedure.
\newblock www.fifa.com/fifa-world-ranking/procedure/men.html, 2008.

\bibitem{asudeh2019designing}
A.~Asudeh, H.~Jagadish, J.~Stoyanovich, and G.~Das.
\newblock Designing fair ranking schemes.
\newblock In {\em SIGMOD}. ACM, 2019.

\bibitem{salimi2019capuchin}
B.~Salimi, L.~Rodriguez, B.~Howe, and D.~Suciu.
\newblock Capuchin: Causal database repair for algorithmic fairness.
\newblock In {\em SIGMOD}. ACM, 2019.

\bibitem{asudeh2016query}
A.~Asudeh, N.~Zhang, and G.~Das.
\newblock Query reranking as a service.
\newblock {\em PVLDB}, 9(11):888--899, 2016.

\bibitem{Gladwell11Order}
M.~Gladwell.
\newblock The order of things: What college rankings really tell us.
\newblock {\em The New Yorker Magazine}, 2011.

\bibitem{propublica}
J.~Angwin, J.~Larson, S.~Mattu, and L.~Kirchner.
\newblock Machine bias: Risk assessments in criminal sentencing.
\newblock {\em ProPublica}, 2016.

\bibitem{friedler2016possibility}
S.~A. Friedler, C.~Scheidegger, and S.~Venkatasubramanian.
\newblock On the (im) possibility of fairness.
\newblock {\em arXiv preprint arXiv:1609.07236}, 2016.

\bibitem{narayanan2018translation}
A.~Narayanan.
\newblock Translation tutorial: 21 fairness definitions and their politics.
\newblock In {\em FAT*}, 2018.

\bibitem{hardt2016equality}
M.~Hardt, E.~Price, and N.~Srebro.
\newblock Equality of opportunity in supervised learning.
\newblock In {\em NIPS}, pages 3315--3323, 2016.

\bibitem{dwork2012fairness}
C.~Dwork, M.~Hardt, T.~Pitassi, O.~Reingold, and R.~Zemel.
\newblock Fairness through awareness.
\newblock In {\em ITCS}, 2012.

\bibitem{edelsbrunner}
H.~Edelsbrunner.
\newblock {\em Algorithms in combinatorial geometry}, volume~10.
\newblock Springer Science \& Business Media, 2012.

\bibitem{redlining}
T.~Jan.
\newblock Redlining was banned 50 years ago. itâ€™s still hurting minorities
  today.
\newblock Washington Post, 2018.

\bibitem{yang2018nutritional}
K.~Yang, J.~Stoyanovich, A.~Asudeh, B.~Howe, H.~Jagadish, and G.~Miklau.
\newblock A nutritional label for rankings.
\newblock In {\em SIGMOD}, pages 1773--1776. ACM, 2018.

\bibitem{asudeh2018obtaining}
A.~Asudeh, H.~Jagadish, G.~Miklau, and J.~Stoyanovich.
\newblock On obtaining stable rankings.
\newblock {\em PVLDB}, 12(3):237--250, 2018.

\bibitem{tang2017determining}
B.~Tang, K.~Mouratidis, and M.~L. Yiu.
\newblock Determining the impact regions of competing options in preference
  space.
\newblock In {\em SIGMOD}, 2017.

\bibitem{montecarlo}
C.~P. Robert.
\newblock {\em Monte carlo methods}.
\newblock Wiley Online Library, 2004.

\bibitem{rrr}
A.~Asudeh, A.~Nazi, N.~Zhang, G.~Das, and H.~Jagadish.
\newblock {RRR}: Rank-regret representative.
\newblock In {\em SIGMOD}. ACM, 2019.

\bibitem{muller1959note}
M.~E. Muller.
\newblock A note on a method for generating points uniformly on n-dimensional
  spheres.
\newblock {\em CACM}, 2(4), 1959.

\bibitem{marsaglia1972choosing}
G.~Marsaglia et~al.
\newblock Choosing a point from the surface of a sphere.
\newblock {\em The Annals of Math. Statistics}, 43(2), 1972.

\bibitem{lucidl1989random}
S.~Lucidl and M.~Piccioni.
\newblock Random tunneling by means of acceptance-rejection sampling for global
  optimization.
\newblock {\em Journal of optimization theory and applications},
  62(2):255--277, 1989.

\bibitem{guan2019mithraranking}
Y.~Guan, A.~Asudeh, P.~Mayuram, H.~Jagadish, J.~Stoyanovich, G.~Miklau, and
  G.~Das.
\newblock Mithraranking: A system for responsible ranking design.
\newblock In {\em SIGMOD}, 2019.

\bibitem{google-gorilla}
M.~Mulshine.
\newblock A major flaw in google's algorithm allegedly tagged two black
  people's faces with the word 'gorillas'.
\newblock Business Insider, 2015.

\bibitem{closed-eyes}
A.~Rose.
\newblock Are face-detection cameras racist?
\newblock Time Business, 2010.

\bibitem{asudeh2019assessing}
A.~Asudeh, Z.~Jin, and H.~Jagadish.
\newblock Assessing and remedying coverage for a given dataset.
\newblock {\em ICDE}, 2019.

\bibitem{google-gorilla-resolution}
A.~Hern.
\newblock Google's solution to accidental algorithmic racism: ban gorillas.
\newblock The Guardian, 2018.

\bibitem{chen2018my}
I.~Chen, F.~D. Johansson, and D.~Sontag.
\newblock Why is my classifier discriminatory?
\newblock In {\em NeurIPS}, 2018.

\bibitem{diversity-jag}
M.~Drosou, H.~Jagadish, E.~Pitoura, and J.~Stoyanovich.
\newblock Diversity in big data: A review.
\newblock {\em Big data}, 5(2), 2017.

\bibitem{biggio2013evasion}
B.~Biggio, I.~Corona, D.~Maiorca, B.~Nelson, N.~{\v{S}}rndi{\'c}, P.~Laskov,
  G.~Giacinto, and F.~Roli.
\newblock Evasion attacks against machine learning at test time.
\newblock In {\em ECML PKDD}, 2013.

\bibitem{sun2019mithralabel}
C.~Sun, A.~Asudeh, H.~V. Jagadish, B.~Howe, and J.~Stoyanovich.
\newblock Mithralabel: Flexible dataset nutritional labels for responsible data
  science.
\newblock In {\em CIKM}. ACM, 2019.

\bibitem{apriori}
R.~Agrawal and R.~Srikant.
\newblock Fast algorithms for mining association rules.
\newblock In {\em VLDB}, 1994.

\bibitem{vazirani2013approximation}
V.~V. Vazirani.
\newblock {\em Approximation algorithms}.
\newblock Springer Science \& Business Media, 2013.

\end{small}
\end{thebibliography}
\end{document}
