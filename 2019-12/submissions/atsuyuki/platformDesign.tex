\section{Platform Design}
\label{sec:platformDesign}

Based on the analysis of the poll of crowd workers
and extensive discussions with participants of the 2019 Shonan Seminar on Future of Work\footnote{ACM Sigmod Blog, Sept. 23, 2019,  \url{http://wp.sigmod.org/?p=2931}},
we have identified platform design as one of the key drivers
for ensuring the continued success of online job platforms.
We first identify a number of issues with the current platform design and make a number of concrete suggestions.

\textbf{Lack of Interoperability:}
The current generation of online job platforms operates in silos.
Even platforms that are in a specific domain such as driving (Uber, Lyft, Ola, Didi, etc.) are not
interoperable with each other.
A driver who has driven more than 10K rides with 4.9 rating in Uber will start
as a newbie when moving to a different platform such as Lyft.
This is also a problem for requesters. Consider a task that requires 10 experts.
It is possible that the labor market has the requisite experts -- but they are scattered across multiple platforms.
In this case, the requester could not successfully complete the task.
By enabling interoperability between platforms such issues and many more could be ameliorated.

\textbf{Lack of Support for Complex Tasks and Workflows:}
Currently, there are a limited number of tasks for which crowdsourcing is possible.
Even sophisticated platforms such as CrowdWorks only support as little as 200 types of tasks.
As more and more task types are being serviced by gig economy,
the need for supporting more complex tasks becomes important.
These complex tasks often have very different set of requirements.
For example, they might require a sophisticated workflow so that output of one stage is passed to another. They could require workers with different types of roles.
There might also be a need for specialized requirements such as splitting
a complex task into multiple sub-tasks that could then be assigned using a workflow.


\textbf{Limited Pricing Model:}
There are four types of pricing  models that are widely prevalent in online job platforms.
First is the fixed income model where each worker is given a fixed amount of money every month or so.
There is also task based pricing where the worker is paid a pre-agreed amount after completing a task.
There are some intermediate approaches such as fixed income plus a bonus amount and discounted pricing for completing large number of tasks.
Finally, there are competitions pricing  models in which we pay to the winner.
For online job platforms to thrive, it is important to have a wider variety of pricing models.

For the remainder of the section, we propose a number of improvements to platforms
that could either reduce the pain points of the key stakeholders or improve their satisfaction.

\subsection{Platform Design for Workers}
Workers are the back bone of a job platform by completing the tasks of requesters.
As discussed in the previous section, the current generation of platforms are not very conducive for
long term employment.

\textbf{Ease of On- and Off-boarding:}
One of the positive things about online job platforms is the flexibility they provide to the worker.
The worker can take tasks that are of interest to them while operating flexible hours.
Many platforms simplify them with as little as some identity document and bank account.
While joining the platform is straightforward, the onboarding process often leaves much to be desired.
Once the worker joins the platform, they are not provided enough guidance to contribute productively.
The worker is expected to learn how to contribute on their own.
Similar to offline employment in a traditional organization, it is important to have a proper onboarding procedure.
The current process for off-boarding is also ad-hoc.
Typically, workers and requesters can easily stop working in a platform.
However, the workers often lose the reputation that they have earned when moving to a different platform.
Similarly, the requesters also lose access to valuable and productive employees when moving to a different platform.
It is important to have a better off-boarding platforms so that the workers could transfer the reputation and knowledge learned from one platform to another.

\textbf{Support for Learning Skills.}
When a worker joins a platform, she often learns ``on-the-job''.
If the worker does not perform well due to inexperience, the job could be rejected by the requester
thereby affecting the approval rate of the worker.
Since a number of requesters filter workers based on task approval rate,
this could limit the number of tasks a new worker could contribute to.
This often leads to frustration of new workers and eventual turnover.
It is often desirable to have a more formal mechanism for simplifying this process.
For example, job platforms could have a collection of previously completed tasks
that could serve as an on-ramp for the workers.
By working on such tasks, the worker can learn the requisite skill in a low stress environment.


\textbf{Knowledge Base (KB) for Workers:}
Currently, there are a number of knowledge repositories in an
enterprise so that employees know the practices of the company.
It is important that online job platforms provide something similar.
Note that this is in addition to the aforementioned set of completed tasks.
As workers finish tasks, they must be able to add things to a personalized knowledge base
about what they learned from the task.
This could be public so that any worker can learn from it or
private where it is visible only to the worker.
As workers become increasingly knowledgeable, this serves as a repository for what they learned over the years.
Of course, this must also be interoperable and associated with the worker and transferable as needed.
This will also allow workers to find other mentors or experts to learn from.

%\scream{I moved Support of AI worker to the next session and added the following paragraph.}

\textbf{Support for Expressing Workers' Preference on Task Assignment}
The task assignment is usually done by workers themselves, partly because automatic assignment of  tasks to workers is not straightforward. There are many reasons for the worker to do the tasks; they did the task because it was easy to do, the task was interesting, they wanted to learn something from the task,  it gave them a lot of money, or it was the regular time slot for the worker to do tasks. If the platform has  the support for them to express their preferences, platforms will be able to automatically suggest them the tasks more accurately.






\subsection{Platform Design for Requesters}
In this subsection, we highlight some of the major pain points of requesters
and how new functionality from platforms could improve their satisfaction.

\textbf{Expressive Specification of Task Requirements.}
Currently, there is limited support from online job platforms for requesters
to precisely specify their requirements.
For example, in AMT, the requester can filter workers based on approval rate but cannot impose additional sophisticated filtering.
It is important for requesters to be able to specify worker requirements such as skills~\cite{DBLP:conf/www/MavridisGM16},
output requirements such as latency, cost and quality.
Furthermore, the requester could be open to various tradeoffs such as cost vs quality / latency.
For example, one might be willing to pay higher for better quality or faster response.
Unfortunately, current platforms do not provide such flexibility.

\textbf{Supporting Complex Tasks and Workflows.}
Almost all of the current job platforms support simple microtasks.
As more and more tasks are disrupted by the gig economy, it is important to have platforms that can support more complex tasks.
Often, complex tasks have a number of distinct requirements.
They are often knowledge intensive and collaborative~\cite{rahman2015task} requiring co-ordination with multiple workers.
They often are not monolithic and must be split into multiple sub-tasks
with different groups of workers completing each of them.
They also often require workers to embrace different roles.
Finally, they often have a complex workflow where the output of one stage is passed to the next.

\textbf{Support for Workflow Evolution and Changes during the Execution.} Workflows sometimes
need to change or evolve during the execution, since completing all tasks requires a long time in
general and it is often the case that we find better workflows after we start to execute the
original workflow. The platforms should support this kind of evolution and changes of workflows
with a minimum amount of effort.

\textbf{Sophisticated Algorithms for Assigning Workers to Tasks.}
Currently, most crowdsourcing platforms do not have
any sophisticated algorithms in matching workers to tasks.
Often, this is done manually by the workers by browsing the list of available tasks.
Other than filtering the pool of workers, requesters do not have much control on which workers perform the tasks.
Despite extensive research in algorithms for task assignment~\cite{basu2015task,ho2012online,rahman2015task},
they are not often incorporated into the platforms.
It is important to either have sophisticated algorithms for task assignment so that requester specifications are satisfied or provide an alternate way for the requesters to select workers.

\textbf{Support for AI Workers.}
Given the increasing capabilities of AI, it is a matter of time when AI workers become a major part of online job platforms.
There are a number of scenarios where AI workers could be useful.
If there are urgent tasks, then it is not always possible to use humans to answer them.
Often, there is a substantial latency when humans are involved.
In these circumstances, AI workers could be a valuable resource.
Alternatively, the requester could be cash-strapped and willing to accept less
accurate results in exchange for cheaper payments.
There are many collaborative situations where AI takes care of the boring work
while the human works on the subset that requires human intuition.
Finally, AI and humans should be able to replace each other in some situations: an AI
can be used as a fallback is a human is answering too late to an urgent task, and
conversely, humans can be used as a fallback if an AI fails at recognizing something critical.
However, one must be careful in how AI workers are integrated.
If not, they could replace human workers causing significant social strife.
Furthermore, it is important for requesters to understand the advantages and limitations of the AI workers.


\textbf{Total Optimization.}
Currently, most of the work on matching is done in a piecemeal manner
where best workers are identified for each task.
It is often important to have a holistic optimization
that takes the preferences of all the stakeholders into account.
This would ensure that good workers are overloaded with work
and the workers/tasks are matched in a fair manner. 

\textbf{Algorithm Boutiques:}
A typical crowdsourcing platform could be improved by incorporating algorithms into the major components including (i) how the task requirements are specified (ii) how tasks are assigned to workers (iii) how the ground truth of tasks are obtained by aggregating worker responses and (iv) how the skills of workers are learned based on their response to tasks.
Unfortunately, most of the platforms do not incorporate any of these algorithms.
Most of these points are offloaded to the workers and requesters.
While experienced requesters often have a concrete mechanisms to effectively achieve each of them,
the vast majority of requesters have an ad-hoc and sub-optimal procedure.
Finally, even if some platforms implement the algorithms, they are often opaque
and the requesters have no say in how they are chosen.
It is important for a crowdsourcing platform to have a boutique of algorithms
from which requesters can choose the specific variants.

\textbf{Bespoke Platforms:}
Most of the crowdsourcing platforms are not very customizable.
For example, a domain scientist might need a custom crowdsourcing platform for the task at hand.
Currently, the scientist is left with two unappealing choices.
Either use an existing platform by approximating the task to suit the constraints of the platform.
Alternatively, the scientist could build a new platform from scratch at tremendous cost.
In order to unleash the future of work, it is important to enable any requester
to create custom online job platforms.
It must have a set of default algorithms that could be customized by the requester.
The emergence of on-demand computing frameworks such as Amazon AWS or Microsoft Azure
lowered the cost of startups by relieving them of the pressure of managing servers.
We believe that the time is ripe to do something analogous for crowdsourcing.

\textbf{Library of Workflows and AI Workers.}
In order to help new requesters, it is important for crowdsourcing platforms to provide a large collection of commonly used workflows.
Similarly, they could also provide some AI workers that could perform limited set of tasks
albeit with the understanding that the work could be of lower quality that of a human.

\textbf{Open Source Academic Platforms.}
Most of the online job platforms are closed source and proprietary. 
This inhibits research on improving various components of the platform and evaluate the potential impact.
One natural solution for driving further research in platform design is through open source academic platforms.
A well designed and modular platform could allow a researcher to modify certain components, investigate how it impacts the stakeholders 
and use to improve platform design.
Currently, the researcher has to more or less implement the end-to-end crowdsourcing platform which could be prohibitively challenging.
There are a number of promising options such as Headwork and Crowd4U. Headwork~\footnote{\url{http://headwork.gforge.inria.fr}} is a proof-of-concept crowdsourcing platform focusing on skill modeling and complex task workflows, using Tuple Artifacts. Crowd4U~\cite{morishima2014crowd4u} is a nonprofit open microvolunteering and crowdsourcing platform for academic and public purposes.
It has been widely used for tasks such as identifying building damages during natural disasters, annotative tweets, translation,
identifying paths of tornados and so on. 
The most appealing property is its ability to extend the functionality through a datalog type language called CyLog~\cite{morishima2012cylog}.
For example, when the authors came up with an algorithm~\cite{rahman2015task} for task assignment in collaborative crowdsourcing setting
they were able to easily implement it on top of Crowd4U~\cite{ikeda2016collaborative}.
Such functionality has the potential to dramatically improve crowdsourcing research on platform design. 
