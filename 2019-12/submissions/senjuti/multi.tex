\vspace{-0.2in}
\section{Optimized Multi-Labels Acquisition Involving Crowd}\label{unlab}
\vspace{-0.1in}
We now investigate the multi-labels acquisition scenario. We are unaware of any related work that performs multi-label acquisition in an active learning settings involving crowd. Although one can transform a multi-label task to several single-label tasks, this simple approach can generate many tasks, incurring a
high cost and latency. Akin to the previous section, our effort is to design solutions that adapt a few recent active learning works~\cite{multi0,multi1,multi2,multi3} for multi-label acquisition and combine that with worker-centric optimization, described in Section~\ref{hf}.

{\bf Objectives:} We will adapt a few known active learning algorithms for multi-label classifications using Support Vector Machine (SVM), Naive Bayes, or Ensemble classifiers~\cite{multi0,multi1,multi2,multi3}. We will combine and augment them with {\em worker-centric optimization through human factors modeling}. Using Example~\ref{ex1}, this is akin to selecting the most appropriate unidentified image of the species and select the most appropriate workers to provide multiple labels. Since a task could be labeled by multiple workers, we will study how to aggregate multiple responses and infer the correct labels (truth inference problem) of a task. We will also explore the use of correlations among different labels to improve the inference quality. Finally, we will investigate the stopping condition or {\em convergence criteria}. 



{\bf Challenges:} Workers may exhibit different characteristics in multi-label tasks: a conservative worker would only select labels that the worker is certain of, while a casual worker may select more labels. To determine the multi-label tasksâ€™ results, the key is to
devise the so-called ``worker model'' to accurately express the behavior of the worker in answering multi-labels. Furthermore, different from single-label tasks, correlations among labels inherently exist in multi-label tasks. For Example~\ref{ex1}, consider one pairwise label dependency: if the insect in the image is labeled as Papilionidae (Family name) , then it is highly probable that it also has label
Swallowtail (Sub-family name). Therefore, how to understand and leverage label correlation is another challenge. Finally, how to systematically design {\em optimization function}, i.e., one that combines worker-centric optimization in active learning settings~\cite{multi0,multi1,multi2,multi3} is the final important challenge.


%An oracle, who knows the ground truth, no longer exists in crowdsourcing; instead, multiple workers, with varying expertise (skill), are available. Under this settings, how to realign traditional active learning goals that are system-centric (i.e., optimizes underlying computational task) requires further investigations. How to systematically design {\em optimization function}, i.e., one that combines worker-centric optimization in traditional active learning settings~\cite{active-learning-cs1,active-learning-cs2} is the second important challenge. An equally arduous challenge is the efficiency issue which is mostly overlooked in the existing research. Finally, when to terminate further label acquisition also needs to be examined.

\vspace{-0.1in}
\subsection{Proposed Directions}
\vspace{-0.1in}
Our overall approach is iterative here as well, where, in each round a set of sub-tasks are selected to be annotated with multi-labels and a set of workers are chosen. Once multiple labels are acquired, the underlying classification model is retrained. After that, either the process terminates or we repeat. It has three primary directions: (1) {\em Task assignment} (2) {\em  Truth Inference, i.e.,  aggregate multiple annotations to obtain the ``true'' labels.} (3) {\em Label Correlation}.

{\em Task Assignment:} In our preliminary investigation, we have studied the active learning problem for the multi-label scenario considering the widely popular SVM classifier using the {\em Maximum-Margin Uncertainty Sampling}. Uncertainty sampling \cite{al1} is one of the simplest and most effective active learning strategies used for single-label classification. The central idea of this strategy is that the active learner should query the instance which the current classifier is most uncertain about. For binary SVM classifiers, the most uncertain instance can be interpreted as the one closest to the classification boundary by selecting the sample with the smallest classification margin. Multi-label active learning methods simply extend this binary uncertainty concept into the multi-label learning scenarios by integrating the binary
uncertainty measures associated with each individual class in independent manners, such as taking the minimum over all classes, and taking the average over all classes.

In our initial direction, given the active learning principle, we combine that with worker-centric optimization and design an objective function akin to Equation~\ref{eqn:eq2}, as described in Section~\ref{label}. Obviously, exploring alternative optimization models, or how to design a set of optimization functions that can handle a variety of scenarios, or when to stop the iterative process are additional challenges. Once we understand these challenges for the single-label acquisition problem in Section~\ref{label}, we believe they will extend for the multi-label scenarios.

{\em  Truth Inference Problem:}
The truth inference problem, i.e, how to aggregate the annotations provided by multiple workers and generate the actual set of labels requires deeper attention for the multi-label scenario. As the correct set of labels associated with each sub-task is unknown (ground-truth is unknown), the accuracy or expertise of a worker can only be estimated based on the collected answer. To model worker expertise, we compute the following two measures, {\em True Positive (TP)} and {\em False Positive (FP)}. TP is the number of labels that a worker selected correctly and FP is the number of labels she selected incorrectly. Unlike a prior work~\cite{zhao2012bayesian}, False Negative and True Negative are not relevant, if the workers annotate the labels. In the case where workers validate the given labels, these latter two measures are also relevant. Once these measures are computed, we design a worker's contingency table and calculate her expertise. After that, we design an iterative approach, which can jointly infer the correct labels associated with the tasks and the expertise of the workers.  Our iterative solution is motivated by the Expectation Maximization (EM) algorithms and comprises of the following two steps: (step 1), we assume that the worker expertise is known and constant, and infer the probabilistic truth of each object and label pair. (step 2), based on the computed probabilistic truth of each object and label pair, we re-estimate workers expertise.

{\em  Label correlation:}
Since the annotated labels of an object are not independent (Recall Example~\ref{ex1} and note that Papilionidae (Family name) and Swallowtail (Sub-family name) are highly correlated), we study how label correlations can be inferred and facilitate truth inference. In our initial direction, we leverage the existing label correlation techniques~\cite{lc1,lc2} to generate the {\em pairwise label correlations} and regard them as prior input to our problem. %Pairwise correlation is computed on a pair of labels, whereas higher order label correlations is among a set of labels. 
For example, the conditional dependency of two labels defines the probability that one label is correct for an object under the condition that the other label is correct. Capturing the higher order label correlations requires computing the joint probability which could be computationally expensive. Once label correlation is computed, we shall explore how to use that information for improved truth inference. 
 \vspace{-0.1in}
\subsection{Open Problems}
\vspace{-0.1in}

\noindent {\bf Alternative Active Learning Strategy Design}
In our initial direction, we have discussed how to adapt uncertainty sampling to design active learning strategies for SVM classifier for multi-label scenario. The average number of correct labels assigned to each instance in a multi-label data set is called its label cardinality. Thus the number of predicted  labels of an unlabeled instance is expected to be consistent with the label cardinality computed on the labeled data. For an unlabeled instance, this inconsistency measure could be defined as the distance between the number of correctly predicted labels so far and the label cardinality of the labeled data. We will study this {\bf label cardinality inconsistency}~\cite{tsoumakas2006multi} to select that sub-task where the label inconsistency is highest.
Additionally, we will also study the active learning strategies known for other classifiers, such as Naive Bayes and Ensemble methods could be adapted to our problem~\cite{multi0,multi1,multi2,multi3}. Alternatively, task selection can be guided by a version space analysis such that it will give rise to maximum reduction in the version space of the classifier~\cite{versionspace}.

\noindent {\bf Truth Inference with Label Correlation} We will study how to use the information obtained from label correlation to improve the truth inference. Intuitively, our truth inference problem could benefit from label correlation in the following way: using Example~\ref{ex1}, if label correlation infers high correlation among two labels, let's say, Papilionidae and Swallowtail (family and sub-family of butterflies), it is likely that  Papilionidae and Mimic Sulfurs (which is a sub-family of butterflies, but Mimic Sulfurs belong to a different family (Pieridae) will have a very low correlation. Therefore, the probabilistic truth of the labels which have Mimic Sulfurs should be downgraded to reflect that fact. It has been shown in Information Retrieval that the more frequent two words occur together in text corpus, the more similar their vectors are~\cite{baeza1999modern}. We will regard each label as a word and compute the similarity (e.g., cosine similarity) between the vectors of two labels. We will explore widely popular Sigmoid function~\cite{ito1992approximation} to map a probability value to a real value, re-scale the value based on label correlation, and then revert the re-scaled correlation back to a probability score using the Sigmoid function again. 


%\noindent {\bf P3.3.3.3 : Relevant Label Sparsity}
%Traditional active learning techniques for multi-label learning do not take into account that the relevant labels are usually sparse~\cite{yang2009effective}. However, in the real world scenario, the average number of relevant labels per data point is small leading to relevant label sparsity. In this open problem, we intend to study how to design active learning strategies considering label sparsity. A related work~\cite{multi3} has developed an alternate inference technique  for  the  sparse  Bayesian  multi-label  graphical  model to carry out efficient mutual information based active learning. The authors have developed an approximation to the mutual information which is tightly coupled to their inference algorithm. This approximation is computed much more efficiently than the exact mutual information. We shall study the applicability of such techniques for our Bayesian model, as well as other classifiers, such as SVM and Ensemble Methods.
