\vspace{-0.2in}
\subsection{Optimized Single-Label Acquisition Involving Crowd}\label{label}
\vspace{-0.1in}
We now investigate our proposed optimization framework for single-label acquisition. This problem is examined by augmenting active learning principles with worker-centric optimization (refer to Section~\ref{hf}).

{\bf Objectives:} We are assuming a setting where single-label acquisition is difficult, expensive, and time consuming (such as, Example~\ref{ex1}). We adapt a set of popular as well as well-known active learning principles\cite{al1,qbc1,qbc2,error-reduction} that are proposed to optimize system-centric cirteria, such as, {\em minimizing uncertainty or maximizing expected error-reduction} that are known to be effective in supervised (classification) algorithms~\cite{al-svm,al-svm2,al-dtree,korner2006multi}. We augment these active learning principles with worker-centric optimization. Given a pool of unlabeled instances  (of sub-tasks) and an available set of workers, the objective is to select sub-tasks for further labeling and assign workers for annotations, such that, the assignment optimizes both system and workers. The same sub-task may be annotated  by multiple workers.

{\bf Challenges:} An oracle, who knows the ground truth, no longer exists in crowdsourcing; instead, multiple workers, with varying expertise (skill), are available. Under this settings, how to realign traditional active learning goals that are system-centric (i.e., optimizes underlying computational task) requires further investigations. How to systematically design {\em optimization function}, i.e., one that combines worker-centric optimization in traditional active learning settings~\cite{active-learning-cs1,active-learning-cs2} is the second important challenge. An equally arduous challenge is the efficiency issue which is mostly overlooked in the existing research. Finally, when to terminate further label acquisition also needs to be examined.

\vspace{-0.1in}
\subsection{ Proposed Directions}
\vspace{-0.1in}
Our overall approach is iterative, where, in each round a set of sub-tasks are selected for annotation and a set of workers are chosen. Once annotations are received, the underlying classification model is retrained. After that, either the process terminates or we repeat. It has three primary directions: (1) {\em in a given round, which sub-tasks are to be selected for annotation and assigned to which workers?} (2) {\em  how to aggregate multiple annotations to obtain the ``true'' label?} (3) {\em when to stop?}  

{\em Which sub-tasks are to be selected and assigned to which workers?} We take a set of well-known active learning techniques, such as, {\em uncertainty sampling \cite{al1}, query-by-committee \cite{qbc1,qbc2}, or expected-error reduction~\cite{error-reduction}, used in popular classification algorithms, such as, Naive Bayes~\cite{entropy}, SVM~\cite{al-svm,al-svm2}, Decision Trees~\cite{al-dtree}, or ensemble classification\cite{korner2006multi}} and study them in crowdsourcing.

When a single classifier with a binary classification task is involved and the classifier is probabilistic (such as Naive Bayes), we consider existing uncertainty sampling~\cite{al1} techniques. We use entropy~\cite{entropy} to model uncertainty to choose that sub-task for labeling whose posterior probability of being positive is closest to $0.5$. For non-probabilistic classifiers (such as SVM or Decision Tree), we explore {\em heterogeneous approach}~\cite{lewis1994heterogeneous}, in which a probabilistic classifier selects sub-tasks for training the non-probabilistic classifier. We also study existing expected-error reduction~\cite{error-reduction} techniques that select the sub-tasks to minimize the expected future error of the supervised algorithm, considering {\em log-loss or $0/1$-loss}. We study the query-by-committee\cite{qbc1,qbc2} technique, we choose that sub-task for further labeling which has the {\em highest disagreement}. 

Active learning principles  mentioned above are too {\em ideal} to be useful in a crowdsourcing platform. A simple alternative is to design a {\em staged solution}, where we first select the tasks and then the workers~\cite{active-learning-cs1}. For us, we can take the task-selection solution from~\cite{active-learning-cs1} and then plug in our worker-centric optimization (Section~\ref{hf}) to compose tasks for the workers. We, however, argue that such a staged solution is {\em sub-optimal}, simply because, tasks selected by {\em active learning} techniques may end up having a very low worker-centric optimization, resulting in poor outcome overall. We therefore propose a global optimization that combines (1) worker-centric goals (recall Equation~\ref{eqn:eq3}). (2) active learning principles considering workers with varying expertise. 

Recall Section~\ref{dm} and note that $q^t$ represents sub-task $t$'s contribution towards a given active learning goal (for example, how much $t$ reduces uncertainty or expected-error) at a given iteration. Let $S^{t_u}$ represent the sub-tasks assigned to $u$ with value 
$V(S^{t_u})$ (recall Equation~\ref{eqn:eq3}). Considering worker's skill $s^{u_t}$ as a probability, $u$'s {\em expected contribution} to $t$ is  $s^{u_t} * q^t$~\cite{clemen2007aggregating}. One possible way to combine them is as a multi-objective global optimization function where the objective is to select sub-tasks and workers that maximize a weighted linear aggregation of worker and task-centric optimization (Equation~\ref{eqn:eq2}, where $W_1,W_2$ are specific weights). While linear aggregation is not the only way, it is more likely to admit efficient solutions, where the weights are tunable by domain experts (by default, $W_1=W_2=0.5$). 


%In our initial direction, we combine them in a weighted linear fashion  considering weights by combining both worker and task-centric criteria, where the latter is modeled as a linear weighted aggregation by multiplying $q^t$ with $s^{u_t}$ ($u$'s skill/accuracy in task $t$).

\begin{equation}\label{eqn:eq2}
 \text{ Maximize } \mathcal{V} =  \sum_{u \in \mathcal{U}} [W_1 * V(S^{t_u}) +  W_2 * \sum_{t \in S^{t_u}} (s^{u_t}*q^t)]
\end{equation}

Additionally, if a task has a cost budget associated that could be assigned either as a constraint to this optimization problem, or we could use cost as another objective as part of the optimization function, akin to one of our recent works~\cite{roy2015task}. Nevertheless, we acknowledge that designing the ``ideal'' optimization model that suffices the need of every application is practically impossible. We address this in the open problems.

{\em  Aggregating multiple annotations:} Another challenge is how to combine annotations from multiple workers with varying expertise to obtain the ``true'' label. We apply weighted majority voting types of approach~\cite{ho2013adaptive}, where the weights are chosen according to the skills of the workers. We also consider iterative algorithm for this purpose. Examples of iterative techniques include EM or Expectation Maximization\cite{hung2013evaluation}. The main idea behind EM is to compute in the $E$ step the probabilities of possible answers to each task by weighting the answers of workers according to their current expertise, and then to compute in the $M$ step re-estimates of the expertise of workers based on the current probability of each answer. The two steps are iterated until convergence.  We explore Bayesian solution~\cite{clemen2007aggregating} to probabilistically obtain the true label, i.e., given workers' annotations and skill, compute $Pr(t=0)$ and $Pr(t=1)$ and choose the one which has the higher probability.

%There exists a number of complex technical open problems.
\vspace{-0.1in}
\subsection{Open Problems}\label{opp}
\vspace{-0.1in}
{\bf  Solving the optimization problem} Solving the optimization problem described above is challenging. In a very recent work, we have formalized  task assignment as a linear combination of task relevance (based on a Boolean match between worker expertise and the skill requirements of a task) and skill-diversity~\cite{edbt172} and proved the problem to be NP-Complete~\cite{feo1990class,feo1992one}. We use Maximum Quadratic Assignment Problem (MAXQAP in short)~\cite{arkin2001approximating} to design an efficient algorithm with approximation factor $1/4$. For our problem, we will examine if it is at all possible to design an objective function (perhaps as a special case) to exploit its nice structural properties, such as, {\em sub-modularity or cancavity}. Such an effort is made for active learning problems recently~\cite{hoi2006batch} without considering human workers. We will also study the possibility of staged algorithms and heuristic solutions, as described above. To make the algorithm computationally efficient, we will examine how to design incremental active learning strategies~\cite{qi2009two}, such as finding the new classification model that is most similar to the previous one, under a set of constraints. 


{\bf Complex function design and stopping condition} We note that the formulation described in Equation~\ref{eqn:eq2} is rather {\em simple} - a linear function may not be adequate to combine worker and task-centric optimization. We will explore non-linear multiplicative functions. Another possible way is to formalize this as a bi-criteria optimization problem and design pareto-optimal solution that does not require us to assign any specific weight to the individual functions~\cite{bilo2004pareto,anagnostopoulos2012online,asudeh2014crowdsourcing}.  Finally, we  will examine {\em when to terminate this iterative process}. For the overall classification task $\mathcal{T}$, when quality threshold is not reached or budget is not exhausted (these are two hard stopping conditions), we will design stopping condition by measuring the  confidence~\cite{vlachos2008stopping} of the classification model, or availability of suitable workers.

{\bf Develop a number of optimization models that are likely to cover a variety of scenarios}  We realize that what constitutes the ``ideal'' optimization model is an extremely difficult problem and highly application dependent (e.g., Which factors are important? Should we add or multiply different human factors? In the case of linear weighting, what should be the weighting coefficients?). Even a domain expert who is very knowledgeable about the specific application may not be able to shed enough light on this. We hope to develop a rich set of different models that will cover the various types of applications. This idea of developing a set of optimization models draw parallels from Web Search and Information Retrieval - where a set of alternative criteria, such as relevance, diversity, and coverage, are considered~\cite{baeza1999modern}. In our case, this is analogous to developing models that only consider workers skills/expertise, or cost, or motivation, or includes a subset of human factors that we are interested to study in this project. 