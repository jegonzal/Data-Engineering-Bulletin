\vspace{-0.2in}
\section{Related Work}
\vspace{-0.1in}



%To support the collection of open-world data, several platforms (e.g., Wikipedia) allow users to decide what data to contribute. Such platforms may collect an unbounded
%amount of valuable data, solely depending on contributor efforts.  The knowledge gain maximization at each question leads, in particular, to user effort minimization. Our motivation for turning to the crowd is similar to that
%of surveys. In surveys, the set of questions is usually small,
%and is chosen in advance, typically by experts \cite{surveybook}.
%However, in an open world, it may be hard to know what to
%ask in advance. 

{\bf Crowdsourced label acquisition:} 
Existing crowdsourcing studies~\cite{single1,single2,single3,single4,single5} focus mainly
on single-label tasks, which require workers to select a single-label
(or choice), e.g., confirm presence or absence of species for Example~\ref{ex1}. In machine-learning, multi-label tasks have been widely studied~\cite{mloc1,mloc2,mloc3} and applied to many applications,e.g., text categorization~\cite{mloc4}, bioinformatics~\cite{mloc5}, albeit in a non-crowdsourcing context. In the context of crowdsourcing, multi-label tasks are
addressed~\cite{single5} based on transforming each task to many independent
single-label tasks, which will incur more latency and budget~\cite{mloc6}, without assuming an active learning settings. Although some recent works~\cite{mloc7,mloc8,mloc9,mloc10} focus on publishing multi-label tasks to crowdsourcing platforms, worker centric optimization is fully ignored there. It is easy to notice that, unlike our goal, explicitly incorporating {\em active learning principles and combining with worker-centric optimization} in selecting tasks and finding workers is not studied in this body of works. 
%Task assignment problems in crowdsourcing are far less studied for labeled and unlabeled data acquisition problems considering system-centric optimization goals, such as, {\em uncertainty sampling or expected error reduction}, which is one of the pivotal aspects of our project. 
%Classical assignment problem solves optimal task allocation
%under complete knowledge of tasks and worker skills.  However,
%crowdsourcing adds a number of challenges such as partial knowledge
%of worker skills, task complexity, budget and adaptiveness to new knowledge, and most importantly human factors.  
%Recent works have begun to explore crowdsourcing specific task
%allocation problems .
%Equally important is the investigation of {\em worker-centric optimization through human factors modeling} that remains largely unexplored in the related work, which we consider as another primary goal.


{\bf Active learning:} Active learning~\cite{al-clus1, al1, al2, al4, qbc1, qbc2, qbc3,error-reduction,model-change, variance-reduction,al-survey,densityweightedmethod,bellare2012active} methods are primarily studied for single-label tasks and attempt to overcome the labeling bottleneck by asking {\em queries} in the form of unlabeled instances to be labeled by an oracle (e.g., a human annotator). The primary assumption is that the label acquisition is {\em costly, although often the
cost is not made explicit}. Cost is modeled more explicitly in utility-base data mining~\cite{provost2005toward} or cost-sensitive learning~\cite{turney2000types}. Repeated labeling~\cite{repeated-labeling} is a related concept that assumes that it is possible to obtain certain (noisy) data values (“labels”) relatively cheaply from multiple sources (“labelers”). These values are then used as training labels. 

Unlike its single-label counterpart, multi-label tasks using active learning is far less studied, except a few recent works~\cite{multi0,multi1,multi2,multi3}. Active learning solutions are designed for Support Vector Machine (SVM), Naive Bayes, or Ensemble classifiers~\cite{multi0,multi1,multi2,multi3} considering multi-abels. Additionally, related work studies the task selection by conducting version space analysis such that it will give rise to maximum reduction in the version space of the classifier~\cite{versionspace}. 

Our work derives its system-centric goals from some of these aforementioned active learning principles and study them in the context of crowdsourcing, modeling {\em human factors}, where {\em worker skill/accuracy} is just one of them.

{\bf Human factors modeling in crowdsourcing:} Worker skill or accuracy is the most well-studied human factors in recent research. The skill estimation problem is connected with the data fusion problem~\cite{survey}, where workers may have domain-specific skill (accuracy) and  may also provide erroneous values. Existing works in crowdsourcing have made an effort to model workers expertise~\cite{chienJuHo,kddcrowd,joglekar2015comprehensive}. Other than skill, understanding demographics and the behavior of the workers in the crowdsourcing platforms has been an interest of recent research~\cite{motiv00,motiv0,motiv1,motiv2,motiv3,motiv4}, albeit mostly {\em in a non-computational manner}. From these body of works, in regards to real motivations of the crowd to participate are identified as follows: the financial reward, the opportunity to develop creative skills, to have fun, to share knowledge, the opportunity to take up freelance work, the love of the community and an addiction to the tasks proposed. However, none of these works makes an effort to model these factors mathematically. Our human factor modeling is motivated from these body of works, but we intend to study them as well-defined optimization problems. In a very recent work, we have proposed optimization models~\cite{edbt172} to combine task relevance and motivation in the task assignment process (albeit in a non-active learning context), whereas, motivation is formalized by considering only the {\em skill-variety} of the tasks. As opposed to modeling motivation as described in Section~\ref{dm}, this recent study only captures motivation \cite{hackman1976motivation} considering skill-variety. A recent survey paper of the PI summarizes these different aspects~\cite{amer2016toward}.

{\bf Active learning involving crowd:} Active learning and crowdsourcing are primarily studied as two independent research directions; only some recent attempts are made to combine these two~\cite{active-learning-cs1,active-learning-cs2,non-active-learning1,mozafari2014scaling, corleone,clamshell,vijayanarasimhan2014large},
that too mainly for single-label tasks. While~\cite{active-learning-cs2} proposes  solutions to identify the most uncertain instances in sub-linear time with a hashing-based solution, it does not consider {\em worker-centric optimization}. To the best of our knowledge, we are unaware of any related work that  attempts to design active learning like techniques involving crowd to acquire multi-labels. 


{\bf Task assignment in crowdsourcing:} 
%Data procurement or sourcing is one of the most important and challenging aspect of crowdsourcing \cite{crowdSurvey}. 
%Some recent works
%(e.g., \cite{DBLP:journals/pvldb/DoanFKK11, DBLP:journals/pvldb/FengFKKMRWX11, DBLP:conf/cidr/MarcusWMM11, DBLP:conf/cidr/ParameswaranP11}) suggest construction of database
%platforms where some components of the database (e.g., values, query parts) are crowdsourced. 
Crowdsourcing has been studied for numerous data sourcing problems~\cite{datasourcing}, such as, entity resolution~\cite{entity-resolution,DBLP:journals/corr/RekatsinasDP15}, query answering~\cite{qp1,qp2}, route planning~\cite{boim2012asking,routeplan}, fact answering~\cite{factanswering}, and so on. The choice of crowd questions take into account monetary cost, latency, accuracy, or different types of constraints \cite{ chienJuHo,kargerBudget,roy2015task}. Related work~\cite{qp2} also deals with deciding which worker should be asked what questions, with the particular goal of minimizing uncertainty, although no supervised or unsupervised algorithm is explicitly assumed in the problem settings. Additionally, prior works have studied the task assignment problem~\cite{qasca,icrowd} by modeling worker expertise, where expertise is modeled as an unknown parameter to be estimated either after the tasks are complete \cite{whitehill,kddcrowd}, or are being completed \cite{vempaty2013reliable, qp1, qp2}. %Different aspects of crowdsourcing related to data procurement is discussed in~\cite{ad}.

\smallskip In summary, we believe that our vision of combining human-centric optimization with active learning principles  and deploying that in a real world online citizen science platform is an worthy goal and has the potential to make significant contributions in cyber human systems.


%One of the characteristics that differentiates the people included in the crowd is
%that they have to be compensated 
%because they are acting voluntarily [34]. Some authors suggest that th
%e best situation would be that in which the reward 
%is not material and that instead the motivation to participate is similar to that in Open Source Communities: passionate 
%about the activity and participating for fun [55]. 
%In regards to real motivations of the crowd to participate, various studies have been carried out [9][51][56]. These 
%studies suggest different motivations that fit some of Maslow’s individual needs: the financial reward, the opportunity 
%to develop creative skills, to have fun, to share knowledge, 
%the opportunity to take up freelance work, the love of the 
%community and an addiction to the tasks proposed; understanding addiction as an exaggeration to describe the amount 
%of time the crowd spends on the crowdsour
%cing site and their love to that site. 
%In this way, the recompense would vary depending on the crowdsourcer, but would always look to satisfy one or 
%more of the individual needs mentioned in Maslow’s pyramid [57]: economic reward, social recognition, self-esteem, 
%or to develop individual skills. Although 
%certain authors such as Kazai [33] also speak of entertainment as a type o
%
%%{\em Composite item exploration:}
%
%%\section*{declarative system design:}




%
