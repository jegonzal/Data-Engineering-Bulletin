\section{Research Directions}\label{sec:research}

Creating a comprehensive benchmark for online job platforms with complex interactions among human workers and requesters, machines, and AI agents is a non-trivial task. Here we list some challenges that need to be addressed when we design such a benchmark.


\textbf{Broad applicability for use in academia and industry.} The benchmarks designed for evaluating the platforms should be able to cover  different types of works, platforms, and workers.  Real applications have different requirements, especially for many real industry applications, therefore, developing a ``leaderboard benchmark'' with single dataset and single application may not work. Instead, we need to support tailored benchmarks for different workloads and have sufficient diversity in the requirements within the benchmark to  satisfy the needs of different platforms, especially from industry (e.g., as done in TPC benchmarks used to evaluate performances of query evaluation supporting different workloads).

\textbf{Acceptability in Research Communities.} One practical challenge after designing a benchmark is to getting the benchmark widely  accepted in the research community and industry. This would need wider discussions and involvement  of all the relevant players like the crowdsourcing platforms from industry  and academia, research communities, and also possibly the workers and requesters who use these platforms.
%(SAT  solver, diversity, coverage of jobs, domain specific)


\textbf{Incorporating Metrics capturing human factors that are not easy to measure.}
One can be seen that a number of metrics like latency, throughput, or cost are easy to measure (although can be measured in multiple ways). On the other hand, some metrics, especially the ones related to human factors such as fairness, equity, and satisfaction, are difficult to measure. Incorporating ideas from the recent advances in research on  fairness and ethics from data management, ML/AI, and also non-computational domains like psychology, cognitive science, sociology, laws, and policy would be useful.

\textbf{Handling multiple criteria and optimizations:} Satisfying multiple metrics all at the same time may lead to multi-objective optimization problems, which are typically computationally intractable. As in computational challenges, formulating the problems meaningfully, designing efficient approximation algorithms with formal guarantees, and also developing efficient tools that produce good results in practice will be required. On the other hand, one can explore whether keeping all the criteria separate and giving individual scores to those criteria work better.

\textbf{Supporting interactions with AI agents:} Since AI agents are used in different stages in an online job platform, we need to develop benchmarks that enable comparison of effectiveness and interactions of AI agents and humans on jobs of various categories. Such a benchmark will have to continuously updated as AI technology improves.

\textbf{Measuring robustness:} Measurement of robustness of the platform or of the matching algorithm used in the platform would require creation of adversarial benchmarks that will enable assessment of effectiveness of integrity checks and anomaly detection mechanisms built into the platforms. For instance, if an adversarial requester attempts to ruin the reputation of a worker with poor ratings, or if a worker attempts to game a system by providing wrong inputs, a good platform would be able to detect or defect such attempts by using other information collected in the process, which might be one of the desired properties of the benchmark.

\textbf{Reproducibility:} Results that use standard benchmarks used in different contexts (e.g., TPC-H or TPC-DS data for evaluating scalability in data management) are repeatable or reproducible. On the other hand, tests involving humans are not always repeatable benchmark tests results could vary for every test run. Allowing some level of differences in the results and considering mean/variances might be needed while developing the benchmarks.

\textbf{Synthetic benchmark data generation:} Based on the data distribution of data logs, such as availability of workers/jobs and dynamics of the platform, collected from real job markets, how can  we generate synthetic  benchmark data which follows the same data distribution under the similar environment scenarios to test various aspects of users, jobs and platforms, such as effectiveness and scalability.
