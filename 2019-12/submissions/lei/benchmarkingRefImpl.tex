%\section{Benchmarking FoW Implementations}
%While metrics is a fundamental aspect of benchmarking,
%there are a number of other dimensions that are equally important.
%In this section, we highling some of them.

%\textbf{Benchmarking Datasets.}
%Crowdsourcing is often considered as an inexpensive mechanism to obtain benchmark datasets.
%Popular datasets for challenging AI tasks such as computer vision and natural language processing
%were obtained through crowdsourcing.
%This allows the requestors to capture human knowledge and intuition
%that is then used to train ML models.
%Hence, it is quite ironic that there is a lack of benchmark datasets for evaluating crowdsourcing research.
%This is a major stumbling blocks in algorithmic research on crowdsourcing.

%FoW platforms have a number of components such as
%task assignment, truth inference from worker responses, estimating human factors and so on.
%Developing benchmarks are often beneficial for both platforms and requestors.
%Not surprisingly, major platforms such as Figure Eight have open sourced a large collection of (small-ish) tasks and their responses~\cite{Figure8DataForEveryone}.
%Similarly, there are some early efforts for obtaining multiple (small) datasets for evaluating algorithms for truth inference~\cite{zheng2017truth}.
%While promising, these are often insufficient.
%It is often desirable to have a single comprehensive dataset that is diverse, large, realistic and has a good mix of easy and challenging tasks.
