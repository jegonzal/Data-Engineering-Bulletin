\section{Benchmarking Data}
\label{sec:benchmarkingData}
With the proposed metrics, the next task is to design Benchmarking data to test the effectiveness of the FoW  platforms. Existing works \cite{yongxin2016online, ChengCY19, ChenCZC19} often use real data set to test. However, it is hard if it is not impossible to derive statistic information of real data. Without knowing the characteristic of real data, it will be difficult to choose the right real data to test. Moreover, even we use all the real data sets, we still do not know if we have tested all the possible worst case scenarios, the robustness of the platform is still unknown. Thus, in this section, we mainly discuss how to design synthetic benchmarking data of workers and tasks to test the FoW platforms.

\subsection{Benchmarking data for workers}

There are many factors we should consider to design benchmarking data of workers, which are listed as follows.

\textbf{Worker's expertise. } 
As we can observe from a crowdsourcing platform, given the same question/task,  different workers may offer different answers. This is because different workers often have different level of expertise in different domains. Thus, when we design benchmarking data of workers, we should assign  workers into different categories (domains) with different expertise levels. Moreover, different category and expertise level distributions should be generated.

\textbf{Worker's preference. } 
Worker's preferences towards different types of tasks also determine the workers' willingness to accept the tasks. For example, some workers prefer image labelling tasks to language translation tasks,  if both types of tasks are available in the platform, with a very high probability, they will choose the image labeling tasks. Thus, benchmarking data of workers should take worker's preferences (categories of tasks) into consideration. Again, we should generate different distributions for category preference of workers.

\textbf{Worker's activeness. } 
Given the same crowdsourcing platform, some workers are quite active and solve many tasks/question within a short period of time, while some only solve a few questions but spend quite long time. Therefore, to generate benchmarking data of workers, we can use  the number of tasks completed with a specified period time as the activeness factor and simulate it with different distributions.


\subsection{Benchmarking data for tasks}

Similar as workers, for tasks, we also need to consider different factors when we want to generate benchmarking tasks.

\textbf{Task's category and difficulty. } Given a crowdsouring platform, there are many different types of tasks. For the same type of tasks, the difficulties are also different. For example, given an image, an image classification task is much easier compared to an object identification task.  Therefore, we need to consider categories and difficulty levels to generate tasks with different distributions.

\textbf{Task's budget. }  Given the same tasks with different budgets,  the task with a higher budget is often accepted much faster than the one with a lower budget. Of course, this does not indicate that tasks with higher budget will get higher quality answers.  We need to generate budgets with different distributions for the tasks.

\textbf{Task's completion time. } When the requester posts a task on the crowdsourcing platform, she often sets up a completion time, which is the time that the requester expects the answer back. Depends on the urgency of the tasks,  different tasks often have different specified completion time. We need to generate completion time with different distributions for the tasks.

\textbf{Tasks' required number of answers. } For some tasks, the requester only needs one answer, such as simple true/false questions, while for complicated tasks, such as object identification, more workers are needed to verify the correctness of labelled objects.  Thus, we need to take tasks' required number of workers as another factor to generate task data.
