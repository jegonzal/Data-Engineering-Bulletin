\section{Introduction}\label{sec:introduction}

Online crowdsouring platforms have become an unavoidable part of our life.
The breadth of these platforms is truly staggering.
These include the widely studied crowdsourcing platforms such as Amazon Mechanical Turk (AMT)
to worker-for-hire platforms such as TaskRabbit.
Even platforms such as Uber that provides ride-hailing services fall under this category.
These alternate task arrangements have been inexorably growing over the last few years.
The crowdsourcing community has done tremendous work in developing algorithms
that enabled productive use of workers/volunteers in important domains
such as Wikipedia, data annotation for machine learning, disaster analysis and so on.
Hence, it is important that our community also takes the lead research on the future of work (FoW) on these online crowdsourcing platforms.
While developing algorithms for specific components is important,
it is much more important to take a holistic perspective and
develop a framework to evaluate the research on this topic.
In this article, we advocate the need for developing benchmarks for crowdsourcing and future of work.

\textbf{Importance of Benchmarks:}
Benchmarks and standardization are an underappreciated type of research
that has the potential to dramatically boost a research domain.
Often, benchmarks have a galvanizing effect on a community by focusing them on the most important things.
This is especially important for emerging areas such as crowdsourcing/FoW.
There are a number of success stories on how developing benchmarks led to the blossoming of a young field.
Two major examples are the fields of databases and deep learning.
In databases, TPC benchmark provides a reference environment in which major
activities of customers (such as transactions or decision support) can be evaluated.
The development of ImageNet benchmark dataset is widely credited as the standard benchmark data to evaluate various of deep learning algorithms on image classification and object identification.
Benchmarks allow researchers to fairly evaluate competing algorithms and spurs innovation.


\textbf{Need for Benchmarks in FoW:}
Online crowdsourcing platforms have been reshaping the workforce.
A number of studies such as~\cite{mcfeely2018workplace} report that
36\% of US workers have an alternative work arrangement in some capacity.
This number has substantially increased over the last few years.
%Online job platforms have a number of commonalities with crowdsourcing platforms.
%Hence, it is natural that the crowdsourcing community takes the lead on research on FoW.
Research on FoW is still at its infancy and it is exactly the time to develop benchmarks
to ensure that the energy of the research community are spent on the major priorities.
There has been extensive work in crowdsourcing
with hundreds of papers being published every year by researchers in
domains as diverse as computer science, psychology, social science, management and so on.
While this allows for cross pollination of ideas in the best case,
it could also cause duplication of work and development of mutually incompatible outcomes.
Benchmarks have the potential to mitigate such efforts.
There are some open data sets in crowdsourcing research such as
Figure Eight's Data for Everyone~\cite{Figure8DataForEveryone}, which contains information about the task (instructions, questions, and answers). There have also been efforts to create a directory of crowdsourcing data sets from various sources~\cite{li2016crowdsourced}.
However, the lack of common metrics and reference implementations makes the problem of identifying
most promising ideas and evaluating competing implementations very challenging.

\textbf{Challenges:}
Developing a benchmark is challenging even in a rigorously empirical field such as databases
where the fundamental metrics for performance are much more well understood.
The biggest strength of crowdsourcing/FoW research, namely its diversity, also causes the biggest challenge.
Unlike databases, FoW could span across many diverse domains and each field could have different metrics and requirements.
Any benchmark should be able to reflect this fundamental property.
Additionally, FoW is uniquely positioned due to the presence of multiple stakeholders
-- workers, requestors and platforms -- who could have different objectives.
Finally, FoW is driven by humans and it is important to take human and social factors into account.
The presence of these volatile human factors~\cite{roy2013crowds}
that are different for each human makes developing uniform benchmarks much more challenging.

\section{Taxonomy for Benchmarks}
Prior approaches such as developing a large dataset such as ImageNet or a synthetic workload such as TPC
are clearly insufficient since we need not only a dataset but also a set of metrics to measuring the effectivenss of the crowdsourcing platforms.
FoW necessitates a fundamental rethink on how benchmarks must be designed.
One of our key contribution is the identification of major dimensions in which benchmarks must be developed.
These include:

\begin{itemize}
    \item \emph{Metrics.}
        Crowdsourcing/FoW has a number of different components such as task assignment,
        collecting and aggregating results from the workers, evaluating worker skills and so on.
        It is important to develop metrics so that crowdsourcing algorithms for these compontents can be fairly evaluated according to these metrics.
        Metrics must measure both accuracy and efficiency.
        These should also reflect the requirements of the stakeholders.
        Almost all of the current metrics are focused on requestors --
        and it is important to flesh out metrics for workers that could include
        human factors~\cite{roy2013crowds} such as job satisfaction and worker fairness.

    \item \emph{Datasets.}
        There is a paucity of datasets that could be used for crowdsourcing research.
        These include all facets such as
        task assignment~\cite{basu2015task,ho2012online},
        identifying ground truth from imperfect responses~\cite{zheng2017truth},
        learning skills of workers~\cite{rahman2015worker},
        identifying spammers~\cite{raykar2012eliminating} and so on.

    \item \emph{Platform Simulations and Synthetic Datasets.}
        In order to evaluate algorithms for platform management tasks (such as matching workers to tasks),
        it is important to have a realistic data that could be used to model workers and tasks.
        These could include their arrival rates, worker demographics and preferences,
        task characteristics and requirements and so on.
        By using these information, it is possible to evaluate task assignment algorithms holistically.
        Of course, these data are often proprietary -- so even a realistic looking synthetic data
        could dramatically improve research.
    \item \emph{Reference Implementations:}
        By standardizing the settings, one could develop reference implementations of various algorithms.
        While there are some early efforts in this direction for truth inference~\cite{zheng2017truth},
        substantial additional work needs to be done.
        These reference implementations allow a researcher to confidently claim that their algorithm
        is better than current state-of-the-art.
    \item \emph{Open Source Online Crowdsourcing Platforms.}
        Currently, Amazon Mechanical Turk is one of the most popular crowdsourcing platform.
        However, no open source clone of it exists.
        It is important to have an open source academic platform that could be used to
        prototype FoW algorithms.
        As an example, the development of Postgres triggered an avalanche of research whereby
        an individual researcher can plug-in their algorithm for specific tasks such as query optimization
        without implementing an database from scratch.
        Currently, a researcher working on task assignment has to build a simulator for a crowdsourcing
        platform to evaluate her algorithm.
        The presence of a modular open source framework allows one to just modify a single component. For example, when the authors implemented an algorithm for task assignment for
        collaborative crowdsourcing~\cite{rahman2015task},
        they were able to implement and evaluate it on the academic platform Crowd4U~\cite{ikeda2016collaborative}. Given another example, when authors want to evaluate various of task assignment algorithms~\cite{yongxin2016online}, they could compare them on the same spatial crowdsourcing platform, gMission~\cite{ChenFZLXCCCTZ14}.
    \item \emph{Competitions for FoW Tasks.}
        The Natural Language Processing community has a tradition of conducting
        yearly competitions (such as SemEval or TREC) for making progress on major research challenges.
        Such events trigger the competitive nature of researchers who strive to beat prior state-of-the-art.
        It is important that our community also adopt this important tradition.
        Every year, one could identify major FoW tasks in important domains such as  Citizen science or disaster crowdsourcing and seek to make meaningful progress.
\end{itemize}
