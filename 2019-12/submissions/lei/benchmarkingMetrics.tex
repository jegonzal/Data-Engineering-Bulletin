\section{Benchmarking Metrics}
\label{sec:benchmarkingMetrics}

As mentioned before, the diversity of the tasks and the presence of multiple stakeholders
makes the process of designing a comprehensive set of metrics very challenging.
In this section, we make an initial attempt in identifying a set of relevant metrics
that are relevant for FoW research.

\textbf{Desiderata for FoW Metrics.}
Metrics are the primary mechanism by which the performance of an FoW platform is evaluated.
Crowdsourcing has been used in a number of diverse domains -- so the metrics must be both generic and comprehensive.
Currently, most of the metrics are focused on the requestors.
It is important to design metrics that take into account the needs of all three major stakeholders -- workers, requestors and platforms.
It should be able to handle both quantitative (such as computational criteria)
and qualitative aspects (such as human and social factors).

\subsection{Metrics for FoW Platforms}
We begin by describing few metrics that could be used to quantify any given FoW platforms.

\textbf{Crowd Size, Diversity and Rate of Participation.}
Workers are an indispensable part of any FoW platform.
So one of the most basic metrics for the platform measures the size of the crowd.
Typically, requestors would prefer a platform with more workers as it gives a wider pool for recruitment.
Of course, size only gives an incomplete perspective of the platform.
The diversity of the crowd is often a better indicator of the quality of worker pool
especially for knowledge intensive crowdsourcing tasks.
A diverse crowd with different background and perspectives
often results in more creative solutions.
Finally, one metric that is useful to measure how thriving a platform is
participation rate that measures the number of workers who are active and perform tasks in a given unit of time such as  a week or a month.
In a thriving FoW platform, one would expect that this number will be large.
An alternate metric could measure the average amount of time that is spent by workers on the platform.

\textbf{Worker Skill Distribution.}
Another key aspect of the FoW platform is the skill distribution of the workers.
In a generic FoW platform such as AMT, each worker and task could be annotated with a set of skills
such as translation, writing, comprehension and so on.
Ideally, requestors would prefer a platform with workers having diverse skills.
Another key aspect is the alignment between the skill distribution
available in the worker pool and the distribution required by the task pool.
Such a misalignment often results in the frustration of both workers and requestors.

\textbf{Task Diversity and Complexity.}
Similarly, it is important to measure the distribution of the tasks themselves.
It is important for the platform to have a wide variety of tasks involving different skills
and varying complexity ranging from easy to difficult.
A steady stream of monotonous or complex tasks could reduce worker motivation and result in turnover.

\textbf{Efficiency Metrics: Tail Latency and Throughput.}
The FoW platform could have a number of quantitative metrics that measure its performance.
Two of the key metrics are latency and throughput.
Latency measures the time taken between posting of a job and its completion.
Of course, some latency is inevitable due to the inherent nature of humans.
However, a large latency would preclude certain tasks that require interactive responses from being posted on the platform.
In addition to the mean and median latencies, it is also important to measure
the tail latencies corresponding to the 95-th or 99-th percentile of task latencies.
Similarly, it is also important to measure throughput which could measure
the number of tasks completed in any given unit of time.
Throughput could also be used to evaluate specific algorithms such as task assignment
wherein it measures the number of tasks for which workers were matched with.

\textbf{Reliability and Robustness to Adversaries.}
Any major FoW platform attracts a wide variety of adversaries
such as workers who are scammers out to make a quick buck by possibly colluding with other workers.
This could also include requestors who either maliciously reject tasks completed by workers so as to not pay them or those who use crowdsourcing for illegal or unethical purposes.
It is important that the platform has sufficient mechanisms so that it is robust against such adversaries.
Crowdsourcing is increasingly being used for major tasks and it is important that the platform is reliable and does not crash.

\textbf{Usability of the FoW Platform Interface.}
An under-appreciated aspect of FoW platforms is how user friendly the interface is.
This is applicable to both the workers and requesters.
Any large FoW platform attracts a diverse group of requestors and workers
who may not be fluent in how the platform works.
For example, the requestors could be domain scientists such as psychologists or social scientists
with limited knowledge of computer science.
Similarly, workers could have a wide variety of educational levels.
Hence, it is important that the platform's interface is intuitive and allows both requestors and workers to complete the tasks efficiently.

\subsection{Metrics for FoW Workers}
Workers are a key part of the platform and its success hinges on learning appropriate information about the workers and use it effectively so that all the stakeholders are satisfied.
We enumerate below a number of key facets of human workers that must be systematized and measured.
Such metrics have the potential to represent the worker holistically than the current approach of quantifying the worker simply as a number based on the task approval rate.

\textbf{Human Factors.}
It is important to model the behavior or characteristics of human workers in any crowdsourcing platform.
This has a number of applications such as assigning appropriate workers to tasks to ensure their completion
and recommending appropriate tasks to workers to increase job satisfaction.
Prior work~\cite{amer2016human,roy2013crowds,cullina2015measuring}
typically involve identifying appropriate human factors,
integrating them into FoW components such as task assignment and
estimating them from past interactions with FoW platforms.
For the reminder of the section, we discuss the major human factors.
Systematically formalizing them and integrating them holistically into FoW platforms is a major open challenge.

\textbf{Skill, Knowledge and Expertise.}
Most generic crowdsourcing platforms such as AMT could have a set of domains
$D=\{d_1, d_2, \ldots, d_m\}$ that denote the various knowledge topics.
Consider a task of translating documents from English to French.
Even this task requires multiple skills such as comprehension of English language,
writing and editing in French.
Generic platforms have a wide variety of task types with large platforms such as Crowdworks
supporting as much as 200 types of tasks.
Given this setting, it is important to enumerate the set of skills needed by the tasks
and possessed by the workers.
One could quantify the skill using categorical values (not knowledgeable, novice, knowledgeable and expert)
or in a continuous $[0, 1]$ scale.
So, a value of $0$ for a specific skill such as English comprehension denotes no expertise
while the value of $1$ could denote complete mastery.

\textbf{Worker Motivation.}
This is one of the most important human factors and a key component of the worker to be measured.
Understanding worker motivations could be used to improve the performance of the FoW platform
through a more informed matching of workers with tasks.
However, understanding what motivates workers is not so obvious.
Some workers could be motivated by monetary compensation while others are motivated by fun and enjoyment.
It has been found that prior work social studies on
workplace motivation is also applicable to FoW platforms~\cite{kaufmann2011more,pilz2013does}.
In fact, there are as many as 13 factors that are highly relevant for worker motivation~\cite{kaufmann2011more}.
These could be categorized as intrinsic and extrinsic motivation.

Intrinsic motivation include aspects of tasks such as~\cite{kaufmann2011more}
skill variety (preference for tasks requiring a diverse collection of skills),
task identity (the worker perception about the completeness of the task),
task autonomy (the degree of freedom allowed during the task),
feedback during the task completion and so on.
Extrinsic motivation includes monetary compensation, human capital advancement and
signaling (performing a task to give a strategic signal to environment).

\textbf{Metrics for Group based Human Factors.}
The increasing popularity of knowledge intensive crowdsourcing tasks requires collaboration.
Hence, it is important to measure the various factors that are relevant to modeling worker collaboration.
The most important of those is worker-worker affinity that measures the collaborative effectiveness of any two workers. This could be extended to measure the social cohesiveness of any group of workers.
It is often desirable to form a group of workers where the aggregate pairwise affinity is large.
Of course, there is a natural diminishing returns when increasing the group size beyond certain threshold dubbed critical mass.


\subsection{Metrics for FoW Tasks}
Tasks (and requestors) form the final leg of FoW platforms.
A number of metrics described for platforms are also applicable for tasks.

\textbf{Accuracy and Quality.}
This is often the most important metric for the requestors.
If the responses of the workers are accurate, then most popular approaches for
aggregating worker responses will provide accurate results.
It is important for the requestor that the completed crowdsourcing tasks have a high accuracy rate.

\textbf{Cost.}
The requestor often wants to complete a given task with high accuracy while minimizing the monetary payment or the worker effort.
There has been extensive work on identifying appropriate workers while satisfying the quality and cost constraints of the tasks.
The requestor has a natural cost-benefit tradeoff and higher cost often deters them without the requisite quality.

\textbf{Completion Rate and Latency.}
The human workers create an uncertainty in terms of task completion.
It is possible that a worker accepts a task but does not complete it immediately.
This creates a straggler effect where some workers could delay the completion of the tasks.
This is especially important for longer tasks where workers losing motivation is a key risk factor.
This affects the requestor in two ways.
First, any task consist of a number of micro-tasks all of which need not be completed.
Higher the completion rate, the better it is for the requestor.
Second, for complex tasks that often have a binary outcome (task completed or not),
it could dramatically increase the latency.
Having systematic method to measure these two phenomenon is quite important.

\textbf{Fairness Related Metrics.}
There has been intensive research on how to quantify fairness.
In our context, it is important to ensure that the platform and the task requestor are seen as fair.
For example, workers who did similar tasks should be paid similar compensation.
Similarly, the worker submissions must not be rejected without a valid reason and so on.
