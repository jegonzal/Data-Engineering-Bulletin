\begin{itemize}
    \item Saravanan: Benchmarking is important -- e.g. imagenet..
    \item so far accuracy or performance in db community
    \item for crowdsourcing, it is not obvious
    \item if anyone does experiments on AMT, e.g., about task assignments, we need to evaluate
    \item also plug in ready-to-use modules
    \item Ria:  needs open source tools for her work
    \item other benchmark papers? our old work?
\end{itemize}

\textbf{Today’s situation: }
The online job platforms available today are widely used, such as crowdflower, crowdworks and AMT, On these platforms, people proposed different incentive mechanisms, job assignment algorithms and quality control solutions. However, there are no comprehensive benchmarks and metrics available to evaluate the efficacy of these mechanisms as well as the platforms.. As an example, a requester who wants a job done through such a platform, may not have guarantees that her jobs are reaching the right set of workers. This may happen when the ranking algorithm used by the platform is not properly designed or not transparent -- then based on the preferences and skill sets entered by the workers, it may display the posted job to the workers who are not suitable or interested, thereby leading to sub-optimal results and dissatisfaction for both the requester and the workers.  Further, without a suitable benchmark, it is hard to know the robustness of the platform. For instance, there exist cases that people use fake IDs to register as workers and use all her registered accounts to complete the jobs, get the reward and run, by cheating the majority voting mechanisms implemented by the platform.

Similarly, many of these platforms critically rely on the star-ratings or reviews of the workers given by the requesters (e.g., Uber, TaskRabbit) while displaying a ranked lists of the workers, therefore, poor ratings given by an  adversarial  requester may ruin the reputation of a worker. Situations like the above motivate the need for designing a comprehensive benchmark and metrics to be used in such a benchmark, such that even if a platform does not satisfy all the desired metrics, the expected output from the platform is transparent to both the requesters and the workers, leading to high quality outputs, desired outcomes, less surprises, and more satisfaction for all the involved players. In addition to industry needs, benchmarks and metrics are urgently needed for academia researchers to conduct comparison study on various algorithms, incentive mechanisms, and quality control solutions. While there are open data sets available [8, 9, 10] in crowdsourcing, there are currently no established benchmarks, difficult for people to choose the solution for different application needs.


\textbf{What do we envision for the future of work:}
The benchmarks and metrics for future of work are required to measure the effectiveness of human and work interaction at various stages, like discovery of jobs and workers, matching of jobs and workers, and also the interactions between the job and the worker. The benchmarks in the discovery phase should be able to evaluate the efficiency of the platform in building a large resource network. The matching phase benchmarks should be able to measure across metrics that measure fairness, equity, accuracy, completion, latency, throughput, cost, and satisfaction from the requester’s and worker’s perspective. The benchmark for the operational phase should be able to assess the effectiveness of the interaction between the human and the job, and also between humans with humans, which may include interactions between requester-worker, or worker with worker (e.g., whether their treatment to others is fair regarding payment and reviews). This also includes the interaction with the artificial intelligence agents that is the buffer between the human and the job.

\red{briefly outline the metrics here}

\textbf{Social Impact:} The metrics and criteria should measure the fairness and equity of the solutions. Equity is important to ensure the system provides access equally to everyone. For example, someone without a smartphone should not be disadvantaged compared to others while using applications like Uber.
Satisfaction is important to assess the future and continued participation of humans in the ecosystem.
The proper implementation of benchmark and metrics can increase the reliability of work produced by platform which can encourage more people to be part of this type of  work ecosystem.


\textbf{Research Impact:}
Like imageNet, the proposed benchmark data will draw immediate attention from the community and research competition will be conducted on proposed benchmark data. The researchers will contribute their effort to improve the comprehensiveness of the benchmark and metrics.
Availability of such a benchmark for social science experiments (e.g., hypothesis testing) will make such experiments more scalable and robust.



\textbf{Platform Impact:}
The proposed benchmarks and metrics will also help the industry to test their platform and improve their product or services. Similar impact has been observed across computation job and scheduling literature. \red{Also see notes on the other google doc. }

%https://docs.google.com/document/d/16hJlzig-EiKbGXxN7MblfqvakbIIynHwZlvuGYfube8/edit
