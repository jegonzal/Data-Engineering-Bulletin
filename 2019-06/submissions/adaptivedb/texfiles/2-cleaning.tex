%!TEX root=../bulletin.tex
\section{Cleaning Data while Discovering Insights}
\label{sec:cleaning}

%The process of gathering, storing, and integrating diverse 
%datasets introduces several inaccuracies in the data:
%Analysts spend 50\%-80\% 
%of their time preparing dirty data before it can be used for information 
%extraction \cite{buckley_new_2012}. 
%Therefore, data cleaning is a major hurdle for data analysis.
%
%Data scientists apply cleaning tasks in an iterative fashion to identify and
%fix inconsistencies until they obtain a clean version of the dataset that
%is ready for the analysis they need to perform. Therefore, data
%cleaning is subjective and time-consuming.

Data cleaning is an interactive and exploratory process which involves expensive operations. 
Error detection requires multiple pairwise comparisons to check the satisfiability of the given constraints \cite{cleanm}.
Data repairing adds an extra overhead since it requires multiple iterations in order to assign candidate values to the
erroneous cells until all constraints are satisfied \cite{nadeef,data_quality,bigdansing,Stonebraker_datacuration}. 
At the same time, data cleaning depends on the analysis that users perform;
data scientists detect inconsistencies, and determine the required data cleaning operations 
while exploring through the dataset \cite{eda}. 
Therefore, the usage of offline data cleaning approaches requires long running times in order to discover and fix the discrepancies that might affect data analysis.

To address the efficiency problem, as well as the subjective nature of data cleaning, there 
is need for a data cleaning approach which
is weaved into the data analysis process, and which also applies data cleaning on-demand. 
Integrating data cleaning with data analysis efficiently supports exploratory data analysis \cite{exploratory_analysis}, and ad-hoc data analysis applications \cite{transform-by-ex} 
by reducing the number and the cost of iterations required in order to extract insights out of dirty data.
In addition, by cleaning data on the fly, one only loads and cleans necessary data thereby minimizing wasted effort whenever only a subset of data is analyzed.

%%By coupling data cleaning with data analysis, we reduce the data cleaning cost by focusing only on 
%%the interesting part of the dataset, that is, the part that the user accesses through the exploratory analysis process.
%To provide correct answers over dirty data, we employ query answer relaxation \cite{online_queryrelax, queryrelax_incomplete}. 
%Query relaxation has been used successfully to address the problem of queries returning no results, 
%or to facilitate query processing over incomplete databases. We define and employ a novel query relaxation technique in the context of 
%dirty data, in order to enhance the query answer with the appropriate information from the dataset that allows the
%detection of violations of integrity constraints.Then, given the detected errors, we propose candidate fixes by providing
%probabilistic answers \cite{probabilistic_dbs}.
%After the execution of each query, we isolate the changes made in the dataset and we apply the delta to the original dataset.
%Thus, we incrementally obtain a clean instance of the dataset by adding an overhead to each query. In addition, we build a
%cost model which decides between incremental cleaning and full cleaning by taking into consideration the type of
%queries, the number of violations of the dataset, and the accuracy of the resulting answer. 
%%We validate our approach by building Daisy, a distributed incremental cleaning framework over Spark \cite{spark}.

We intermingle cleaning integrity constraint violations \cite{dependencies} with exploratory data analysis, in order to gradually clean the dataset. 
Specifically, given a query and a dirty dataset, we use two levels of processing to correctly execute the query
by taking into consideration the existence of inconsistencies in the underlying dataset. In the first level, we map the query to a logical plan 
which comprises both query and cleaning operators. The logical plan
takes into consideration
the type of the query (e.g, Select Project, Join), and the constraints that the dataset needs to satisfy in order to optimally place the cleaning operators inside the query plan. 
Then, in the second level, the logical plan is executed by applying the cleaning tasks that are needed. To execute the plan, we employ a query answer relaxation technique, which enhances the answer of the query with
extra information from the dataset in order to 
detect violations based on the output of each query operator that is affected by a constraint. 
Then, given the detected violations, we transform the query answer into a probabilistic
answer by replacing each erroneous value with the set of values that represent candidate fixes for that value.
In addition, we accompany each candidate value with the corresponding probability of being the correct value of the erroneous cell. 
After cleaning each query answer, the system extracts the changes made to the erroneous
tuples, and updates the original dataset accordingly. By applying the changes after each
query, we can gradually clean the dataset.

\subsection{Logical-level Optimizations} 
In the first stage, the system translates the query into a logical plan involving query and
cleaning operators. The cleaning operators are update operators which either operate over the underlying 
dataset, or over the condition that exists below them in the query plan.
To place the cleaning operators, the system determines whether it is more efficient and/or accurate to integrate the 
query with the cleaning task, and partially clean the dataset, or to fully clean the dataset before executing the query. To decide on the cleaning strategy, 
we employ a cost model which exploits statistics regarding the type and frequency of the violations.
To optimally place the cleaning operators, the system examines: a) the approximate number of violations that exist in the dataset, and 
b) how the query operators overlap with the erroneous attributes. Thus, the statistics provide an estimate of the overhead that the cleaning task adds to each query, and determine the optimal placement of the cleaning operations.


At the logical plan level, we apply a set of optimizations by pruning unnecessary cleaning checks,
and unnecessary query operators. To apply the optimizations, we
analyze how the input constraints that must hold in the dataset affect the query result.
For example, it is redundant to apply a cleaning task in the case of a query that contains a filter condition over a clean attribute.
Therefore, the logical plan will select the optimal execution strategy of the queries given the cleaning tasks that need to be applied.

\subsection{Relaxed query execution}
In the final stage, the system executes the optimized logical plan, and computes
a correct query answer by applying the cleaning tasks at query execution. 
Regardless of the type of query, we need to enhance the query answer with extra tuples from the dataset to allow the detection and repairing of errors.
Executing queries over dirty data might result in wrong query answers \cite{incomplete_dbs}; a tuple might erroneously satisfy a query and appear in the query answer
due to a dirty value, or similarly, it might be missing from the query answer due to a dirty value. 

To provide correct answers over dirty data, we employ query answer relaxation \cite{online_queryrelax, queryrelax_incomplete}. 
Query relaxation has been used successfully to address the problem of queries returning no results, 
or to facilitate query processing over incomplete databases. We define and employ a novel query answer relaxation technique in the context of querying
dirty data, which enhances the query answer with extra tuples from the dataset that allow the
detection of violations of integrity constraints. Then, given the detected errors, we propose candidate fixes by providing
probabilistic answers \cite{probabilistic_dbs}. The probabilities are computed based on the frequency that each candidate value appears - other schemes to infer the probabilities are also applicable.
The purpose of the query answer relaxation mechanism is to enhance the query answer with the required information from the dataset, in order to allow correct answers to the queries. 

To capture errors in query results, we first compute the dirty query answer, and then relax it by bringing extra tuples from the dataset; the extra tuples, together with the tuples of the query answer represent the candidates for satisfying the query. The set of extra tuples consist of tuples which are similar to the ones belonging to the query answer; the similarity depends on the correlation that the tuples have with respect to the integrity constraints that hold in the dataset \cite{scare}. 
After enhancing the query answer with the extra tuples, the cleaning process detects for violations and computes the set of
candidate values for each erroneous cell together with their probabilities. 

By integrating data cleaning with query execution using the aforementioned two-level process, we minimize the cost of data preparation; we efficiently clean only the part of the dataset that is accessed by the queries. In addition, by providing
probabilistic answers for the erroneous entities, we reduce human effort, since users can select the correct values among the set of candidate values over the answers of the queries.

%In a nutshell, based on the 
%logical plan, when the system checks a condition (e.g., \textit{select} or \textit{join} predicate), it enhances the result of the operator with extra tuples from the dataset in order
%to allow the error detection and data repairing operations.  


