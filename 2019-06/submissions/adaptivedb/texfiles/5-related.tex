%!TEX root=../bulletin.tex
\section{Related Work}
\label{sec:related}

Our philosophy has been inspired by the omnipresent work on minimizing data-to-insight time.
In-situ processing approaches, such as the work by Idreos et al. \cite{Idreos2011a} propose adaptive and incremental loading
techniques in order to eliminate the preparation phase before query execution. NoDB \cite{Alagiannis2015} advances this idea by making raw files first-class citizens. NoDB introduces data structures to adaptively index and cache raw files, tightly integrates adaptive loads, while implementing in-situ access into a modern DBMS. 
In the context of processing heterogeneous raw data,
Spark and Hadoop-based systems \cite{apache_drill,spark} operate over raw data, while also supporting heterogeneous data formats.
RAW \cite{Karpathiotakis2014} allows queries over heterogeneous raw files using code generation.
ViDa~\cite{Karpathiotakis2015} envisions effortlessly abstracting data out of its form and manipulating it regardless of its structure, in a uniform way.

Work on reducing the data cleaning cost by automating and optimizing common cleaning tasks significantly reduces human effort and minimizes the preprocessing cost. BigDansing \cite{bigdansing} is a scale-out
data cleaning system which addresses performance, and ease-of-use issues in the presence of duplicates and integrity constraint violations.
Tamr, the commercial version of Data Tamer \cite{Stonebraker_datacuration}, focuses on duplicate elimination
by employing blocking and classification techniques in order to efficiently detect and eliminate duplicate pairs.
In the context of adaptive and ad-hoc cleaning QuERy \cite{quERy} intermingles duplicate elimination with Select Project, and Join queries in order to clean only the data that is useful for the queries.
Transform-Data-by-Example \cite{transform-by-ex} addresses the problem of allowing on-the-fly transformations - a crucial part of data preparation.

Work on adaptive tuning focuses on incrementally refining indexes 
while processing queries. Database Cracking approaches 
\cite{Idreos2011} operate over column-stores and incrementally sort 
the index column according to the incoming workload, thus reducing 
memory access. COLT \cite{Schnaitter2006} continuously monitors the 
workload and periodically creates new indexes and/or drops unused 
ones by adding an overhead to each query.

A plethora of research topics on approximate query processing is also relevant to our work. Offline sampling strategies \cite{Agarwal2013,Chaudhuri2001b} focus on computing the best set of uniform and stratified samples given a storage budget by 
assuming some a priori knowledge of the workload. Online sampling approaches such as Quickr \cite{quickr} take samples during
query execution by injecting samplers inside the query plan.

