\section{Conclusions}
\label{sec:conc}

% discuss Data Sheets and Model Cards further
%are we ready for standards?

In this paper we discussed work on transparency and interpretability for data and models based on the concept of a nutritional label. We presented \rf, a system that automatically derives nutritional labels for rankings, and outlined directions for ongoing research that casts the creation of nutritional labels as a computational problem, rather than as purely a design problem.  

We advocate interpretability tools for a variety of datasets and models, for a broad class of application domains, and to accommodate the needs of a variety of stakeholders.  These tools must be informed by an understanding of how humans perceive algorithms and the decisions they inform, including issues of trust and agency to challenge or accept an algorithm-informed decision.  These tools aim to reduce bias and errors in deployed models by preventing the use of an inappropriate dataset or model at design time.  Although the extent of data misuse is difficult to measure directly, we can design experiments to show how well nutritional labels inform usage decisions, and design the tools accordingly.  More broadly, we see the review of human-curated and machine-computed metadata as a critical step for interpretability in data science, which can lead to lasting progress in the use of machine-assisted decision-making in society.

%, to help answer the following classes of questions:

%\begin{itemize}
%\item {\em Explaining semi-synthetic datasets:}  How do we convey the divergences from the real data introduced by bias-correction~\cite{DBLP:conf/sigmod/SalimiRHS19} and coverage enhancement methods~\cite{DBLP:conf/icde/AsudehJJ19}? The methods underlying a semi-synthetic data involve  a number of assumptions (\eg privacy budget in differential privacy mechanisms, the choice of admissible variables for bias-correction~\cite{DBLP:conf/sigmod/SalimiRHS19}). How do we communicate these assumptions and their effect on interpretation?   

%\item {\em Explaining the intended use:} What queries and analysis tasks does the dataset support faithfully, and with what expected utility?  Under what circumstances will a model trained on this data be unreliable? For example, a model may be known to perform poorly for unerrepresented groups despite good overall accuracy~\cite{DBLP:conf/icde/AsudehJJ19}.  How can such information help in understanding the ``unreliabile zones'' where a ML model should not be used?

%\item {\em Labels for query results:} Labels for training sets constructed from queries over synthetic relational databases require careful design: the union of labels for the underlying tables is neither necessary nor sufficient. For example, even if the student table is a representative sample of the population by race, a join between students and financial aid awardees may not be.    
%\end{itemize}


% in close collaboration between domain experts, academic and industry researchers in data science and AI, and legal and policy scholars.  

 %The result of this work will be a framework that drills in on the reasons for interpretability, and develops effective and actionable explanations for a diversity of stakeholders, recognizing their unique needs. 
