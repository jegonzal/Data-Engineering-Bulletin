@inproceedings{tfx,
 author = {Baylor, Denis and Breck, Eric and Cheng, Heng-Tze and Fiedel, Noah and Foo, Chuan Yu and Haque, Zakaria and Haykal, Salem and Ispir, Mustafa and Jain, Vihan and Koc, Levent and Koo, Chiu Yuen and Lew, Lukasz and Mewald, Clemens and Modi, Akshay Naresh and Polyzotis, Neoklis and Ramesh, Sukriti and Roy, Sudip and Whang, Steven Euijong and Wicke, Martin and Wilkiewicz, Jarek and Zhang, Xin and Zinkevich, Martin},
 title = {TFX: A TensorFlow-Based Production-Scale Machine Learning Platform},
 booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 series = {KDD '17},
 year = {2017},
 isbn = {978-1-4503-4887-4},
 location = {Halifax, NS, Canada},
 pages = {1387--1395},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/3097983.3098021},
 doi = {10.1145/3097983.3098021},
 acmid = {3098021},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {continuous training, end-to-end platform, large-scale machine learning},
}

@inproceedings{modeldb,
 author = {Vartak, Manasi and Subramanyam, Harihar and Lee, Wei-En and Viswanathan, Srinidhi and Husnoo, Saadiyah and Madden, Samuel and Zaharia, Matei},
 title = {ModelDB: A System for Machine Learning Model Management},
 booktitle = {Proceedings of the Workshop on Human-In-the-Loop Data Analytics},
 series = {HILDA '16},
 year = {2016},
 isbn = {978-1-4503-4207-0},
 location = {San Francisco, California},
 pages = {14:1--14:3},
 articleno = {14},
 numpages = {3},
 url = {http://doi.acm.org/10.1145/2939502.2939516},
 doi = {10.1145/2939502.2939516},
 acmid = {2939516},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@inproceedings{lime,
 author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
 title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
 booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 series = {KDD '16},
 year = {2016},
 isbn = {978-1-4503-4232-2},
 location = {San Francisco, California, USA},
 pages = {1135--1144},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2939672.2939778},
 doi = {10.1145/2939672.2939778},
 acmid = {2939778},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {black box classifier, explaining machine learning, interpretability, interpretable machine learning},
}

@Misc{gpyopt,
  author =   {{The GPyOpt authors}},
  title =    {GPyOpt: A Bayesian Optimization framework in Python},
  howpublished = {\url{http://github.com/SheffieldML/GPyOpt}},
  year = {2016}
}


@article{hyperopt,
  author={James Bergstra and Brent Komer and Chris Eliasmith and Dan Yamins and David D Cox},
  title={Hyperopt: a Python library for model selection and hyperparameter optimization},
  journal={Computational Science and Discovery},
  volume={8},
  number={1},
  pages={014008},
  url={http://stacks.iop.org/1749-4699/8/i=1/a=014008},
  year={2015},
  abstract={Sequential model-based optimization (also known as Bayesian optimization) is one of the most efficient methods (per function evaluation) of function minimization. This efficiency makes it appropriate for optimizing the hyperparameters of machine learning algorithms that are slow to train. The Hyperopt library provides algorithms and parallelization infrastructure for performing hyperparameter optimization (model selection) in Python. This paper presents an introductory tutorial on the usage of the Hyperopt library, including the description of search spaces, minimization (in serial and parallel), and the analysis of the results collected in the course of minimization. This paper also gives an overview of Hyperopt-Sklearn, a software project that provides automatic algorithm configuration of the Scikit-learn machine learning library. Following Auto-Weka, we take the view that the choice of classifier and even the choice of preprocessing module can be taken together to represent a single large hyperparameter optimization problem . We use Hyperopt to define a search space that encompasses many standard components (e.g. SVM, RF, KNN, PCA, TFIDF) and common patterns of composing them together. We demonstrate, using search algorithms in Hyperopt and standard benchmarking data sets (MNIST, 20-newsgroups, convex shapes), that searching this space is practical and effective. In particular, we improve on best-known scores for the model space for both MNIST and convex shapes. The paper closes with some discussion of ongoing and future work.}
}




@InProceedings{sacred,
  author    = { {K}laus {G}reff and {A}aron {K}lein and {M}artin {C}hovanec and {F}rank {H}utter and {J}\"urgen {S}chmidhuber },
  title     = { {T}he {S}acred {I}nfrastructure for {C}omputational {R}esearch },
  booktitle = { {P}roceedings of the 16th {P}ython in {S}cience {C}onference },
  pages     = { 49 - 56 },
  year      = { 2017 },
  editor    = { {K}aty {H}uff and {D}avid {L}ippa and {D}illon {N}iederhut and {M} {P}acer },
  doi       = { 10.25080/shinma-7f4c6e7-008 }
}

@misc{ONNX,
author={{ONNX Group}},
title={{ONNX}},
howpublished={\url{https://onnx.ai}}
}


@misc{michelangelo,
author={Jeremy Hermann and Mike Del Balso},
title={Meet {Michelangelo}: Uber’s Machine Learning Platform},
howpublished={\url{https://eng.uber.com/michelangelo/}}
}

@misc{fblearner,
author={Jeffrey Dunn},
title={Introducing {FBLearner Flow}: Facebook’s {AI} backbone},
howpublished={\url{https://code.fb.com/core-data/introducing-fblearner-flow-facebook-s-ai-backbone/}}
}

@misc{tensorboard,
author={Google},
title={TensorBoard: Visualizing Learning},
howpublished={\url{https://www.tensorflow.org/guide/summaries_and_tensorboard}}
}

@article{coleman2017dawnbench,
  title={{DAWNBench: An End-to-End Deep Learning Benchmark and Competition}},
  author={Coleman, Cody and Narayanan, Deepak and Kang, Daniel and Zhao, Tian and Zhang, Jian and Nardi, Luigi and Bailis, Peter and Olukotun, Kunle and R{\'e}, Chris and Zaharia, Matei},
  journal={NIPS ML Systems Workshop},
  year={2017}
}

@misc{baker2016crisis,
    title={{1,500 Scientists Lift the Lid on Reproducibility}},
    author={Monya Baker},
    editor={Springer Nature},
    howpublished={\url{https://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970}},
    month={May},
    year={2016}
}

@misc{joellepinaeu,
    title={{Reproducibility, Reusability, and Robustness in Deep Reinforcement Learning}},
    author={Joelle Pineau},
    year={2018},
    note={ICLR},
    howpublished={\url{https://www.youtube.com/watch?v=Vh4H0gOwdIg}}
}

@book{pmml,
 author = {Guazzelli, Alex and Lin, Wen-Ching and Jena, Tridivesh},
 title = {PMML in Action: Unleashing the Power of Open Standards for Data Mining and Predictive Analytics},
 year = {2012},
 isbn = {1470003244, 9781470003241},
 edition = {2nd},
 publisher = {CreateSpace},
 address = {Paramount, CA},
}

@inproceedings{abadi2016tensorflow,
  title={{TensorFlow: A System for Large-Scale Machine Learning}},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={OSDI},
  volume={16},
  pages={265--283},
  year={2016}
}

@article{chetlur2014cudnn,
  title={{cuDNN: Efficient Primitives for Deep Learning}},
  author={Chetlur, Sharan and Woolley, Cliff and Vandermersch, Philippe and Cohen, Jonathan and Tran, John and Catanzaro, Bryan and Shelhamer, Evan},
  journal={arXiv preprint arXiv:1410.0759},
  year={2014}
}

@inproceedings{chilimbi2014project,
  title={{Project Adam: Building an Efficient and Scalable Deep Learning Training System.}},
  author={Chilimbi, Trishul M and Suzue, Yutaka and Apacible, Johnson and Kalyanaraman, Karthik},
  booktitle={OSDI},
  volume={14},
  pages={571--582},
  year={2014}
}

@inproceedings{dean2012large,
  title={{Large Scale Distributed Deep Networks}},
  author={Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Senior, Andrew and Tucker, Paul and Yang, Ke and Le, Quoc V and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1223--1231},
  year={2012}
}

@inproceedings{jia2014caffe,
  title={{Caffe: Convolutional Architecture for Fast Feature Embedding}},
  author={Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
  booktitle={Proceedings of the 22nd ACM International Conference on Multimedia},
  pages={675--678},
  year={2014},
  organization={ACM}
}

@article{burger2017microsoft,
  title={{Microsoft unveils Project Brainwave for Real-time AI}},
  author={Burger, Doug},
  journal={Microsoft Research, Microsoft},
  volume={22},
  year={2017}
}

@inproceedings{han2016eie,
  title={{EIE: Efficient Inference Engine on Compressed Deep Neural Networks}},
  author={Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A and Dally, William J},
  booktitle={Computer Architecture (ISCA), 2016 ACM/IEEE 43rd Annual International Symposium on},
  pages={243--254},
  year={2016},
  organization={IEEE}
}

@inproceedings{jouppi2017datacenter,
  title={{In-datacenter Performance Analysis of a Tensor Processing Unit}},
  author={Jouppi, Norman P and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and others},
  booktitle={Proceedings of the 44th Annual International Symposium on Computer Architecture},
  pages={1--12},
  year={2017},
  organization={ACM}
}

@article{pena2017benchmarking,
  title={{Benchmarking of CNNs for Low-Cost, Low-Power Robotics Applications}},
  author={Pena, Dexmont and Forembski, Andrew and Xu, Xiaofan and Moloney, David},
  year={2017}
}

@inproceedings{de2017understanding,
  title={{Understanding and Optimizing Asynchronous Low-precision Stochastic Gradient Descent}},
  author={De Sa, Christopher and Feldman, Matthew and R{\'e}, Christopher and Olukotun, Kunle},
  booktitle={Proceedings of the 44th Annual International Symposium on Computer Architecture},
  pages={561--574},
  year={2017},
  organization={ACM}
}

@inproceedings{harlap2016addressing,
  title={{Addressing the Straggler Problem for Iterative Convergent Parallel ML}},
  author={Harlap, Aaron and Cui, Henggang and Dai, Wei and Wei, Jinliang and Ganger, Gregory R and Gibbons, Phillip B and Gibson, Garth A and Xing, Eric P},
  booktitle={Proceedings of the Seventh ACM Symposium on Cloud Computing},
  pages={98--111},
  year={2016},
  organization={ACM}
}

@inproceedings{recht2011hogwild,
  title={{Hogwild: A Lock-free Approach to Parallelizing Stochastic Gradient Descent}},
  author={Niu, Feng and Recht, Benjamin and Re, Christopher and Wright, Stephen},
  booktitle={Advances in Neural Information Processing Systems},
  pages={693--701},
  year={2011}
}

@article{zhang2014dimmwitted,
  title={{Dimmwitted: A Study of Main-memory Statistical Analytics}},
  author={Zhang, Ce and R{\'e}, Christopher},
  journal={Proceedings of the VLDB Endowment},
  volume={7},
  number={12},
  pages={1283--1294},
  year={2014},
  publisher={VLDB Endowment}
}

@inproceedings{glorot2011deep,
  title={{Deep Sparse Rectifier Neural Networks}},
  author={Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  booktitle={Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  pages={315--323},
  year={2011}
}

@article{goyal2017accurate,
  title={{Accurate, Large Minibatch SGD: Training ImageNet in 1 hour}},
  author={Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal={arXiv preprint arXiv:1706.02677},
  year={2017}
}

@article{iandola2016squeezenet,
  title={{SqueezeNet: AlexNet-level Accuracy with 50x Fewer Parameters and $< 0.5$ MB Model Size}},
  author={Iandola, Forrest N and Han, Song and Moskewicz, Matthew W and Ashraf, Khalid and Dally, William J and Keutzer, Kurt},
  journal={arXiv preprint arXiv:1602.07360},
  year={2016}
}

@article{kingma2014adam,
  title={{Adam: A Method for Stochastic Optimization}},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={ICLR},
  year={2015}
}

@inproceedings{sohl2014fast,
  title={{Fast Large-scale Optimization by Unifying Stochastic Gradient and Quasi-Newton Methods}},
  author={Sohl-Dickstein, Jascha and Poole, Ben and Ganguli, Surya},
  booktitle={International Conference on Machine Learning},
  pages={604--612},
  year={2014}
}

@article{sun2017meprop,
  title={{meProp: Sparsified Back Propagation for Accelerated Deep Learning with Reduced Overfitting}},
  author={Sun, Xu and Ren, Xuancheng and Ma, Shuming and Wang, Houfeng},
  journal={arXiv preprint arXiv:1706.06197},
  year={2017}
}

@inproceedings{sutskever2013importance,
  title={{On the Importance of Initialization and Momentum in Deep Learning}},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International Conference on Machine Learning},
  pages={1139--1147},
  year={2013}
}

@inproceedings{adolf2016fathom,
  title={{Fathom: Reference Workloads for Modern Deep Learning Methods}},
  author={Adolf, Robert and Rama, Saketh and Reagen, Brandon and Wei, Gu-Yeon and Brooks, David},
  booktitle={Workload Characterization (IISWC), 2016 IEEE International Symposium on},
  pages={1--10},
  year={2016},
  organization={IEEE}
}

@article{bahrampour2015comparative,
  title={{Comparative Study of Deep Learning Software Frameworks}},
  author={Bahrampour, Soheil and Ramakrishnan, Naveen and Schott, Lukas and Shah, Mohak},
  journal={arXiv preprint arXiv:1511.06435},
  year={2015}
}

@misc{baidu2017deepbench,
  author = {{Baidu}},
  title = {{DeepBench: Benchmarking Deep Learning Operations on Different Hardware}},
  shorttitle = {{DeepBench}},
  howpublished= {\url{https://github.com/baidu-research/DeepBench}},
  urldate = {2017-08-29},
  year = {2017}
}

@misc{chintala2017convnet,
  title = {{Convnet-Benchmarks: Easy Benchmarking of All Publicly Accessible Implementations of Convnets}},
  copyright = {MIT},
  shorttitle = {convnet-benchmarks},
  howpublished= {\url{https://github.com/soumith/convnet-benchmarks}},
  urldate = {2017-09-02},
  author = {Chintala, Soumith},
  month = sep,
  year = {2017},
}

@misc{google2017benchmarks,
  author = {{Google}},
  title = {{TensorFlow Benchmarks}},
  howpublished= {\url{https://www.tensorflow.org/performance/benchmarks}},
  urldate = {2017-08-29},
  journal = {TensorFlow},
  year = {2017}
}

@inproceedings{shi2016benchmarking,
  title={{Benchmarking State-of-the-Art Deep Learning Software Tools}},
  author={Shi, Shaohuai and Wang, Qiang and Xu, Pengfei and Chu, Xiaowen},
  booktitle={Cloud Computing and Big Data (CCBD), 2016 7th International Conference on},
  pages={99--104},
  year={2016},
  organization={IEEE}
}

@inproceedings{krizhevsky2012imagenet,
  title={{ImageNet Classification with Deep Convolutional Neural Networks}},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1097--1105},
  year={2012}
}

@inproceedings{lin2014microsoft,
  title={{Microsoft COCO: Common Objects in Context}},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={European Conference on Computer Vision},
  pages={740--755},
  year={2014},
  organization={Springer}
}

@article{rajpurkar2016squad,
  title={{SQuAD: 100,000+ Questions for Machine Comprehension of Text}},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}

@article{han2015deep,
  title={{Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding}},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={arXiv preprint arXiv:1510.00149},
  year={2015}
}

@article{krizhevsky2009learning,
  title={{Learning Multiple Layers of Features from Tiny Images}},
  author={Krizhevsky, Alex and Hinton, Geoffrey},
  year={2009},
  publisher={Citeseer}
}

@article{jia2017adversarial,
  title={{Adversarial Examples for Evaluating Reading Comprehension Systems}},
  author={Jia, Robin and Liang, Percy},
  journal={arXiv preprint arXiv:1707.07328},
  year={2017}
}

@inproceedings{clipper,
 author = {Crankshaw, Daniel and Wang, Xin and Zhou, Giulio and Franklin, Michael J. and Gonzalez, Joseph E. and Stoica, Ion},
 title = {Clipper: A Low-latency Online Prediction Serving System},
 booktitle = {Proceedings of the 14th USENIX Conference on Networked Systems Design and Implementation},
 series = {NSDI'17},
 year = {2017},
 isbn = {978-1-931971-37-9},
 location = {Boston, MA, USA},
 pages = {613--627},
 numpages = {15},
 url = {http://dl.acm.org/citation.cfm?id=3154630.3154681},
 acmid = {3154681},
 publisher = {USENIX Association},
 address = {Berkeley, CA, USA},
}

@misc{howard2018training,
  author={Jeremy Howard},
  title={{Training ImageNet in 3 hours for \$25; and CIFAR10 for \$0.26}},
  howpublished={\url{http://www.fast.ai/2018/04/30/dawnbench-fastai/}},
  year={2018}
}

@misc{codalab,
  author={Percy Liang and others},
  title={{CodaLab}},
  howpublished={\url{https://worksheets.codalab.org}},
  year={2018}
}

@misc{binder,
  key={binder},
  title={{Binder}},
  howpublished={\url{https://mybinder.org}},
  year={2018}
}

@inproceedings{he2016deep,
  title={{Deep Residual Learning for Image Recognition}},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={770--778},
  year={2016}
}

@article{real2018regularized,
  title={{Regularized Evolution for Image Classifier Architecture Search}},
  author={Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V},
  journal={arXiv preprint arXiv:1802.01548},
  year={2018}
}

@article{micikevicius2017mixed,
  title={{Mixed Precision Training}},
  author={Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaev, Oleksii and Venkatesh, Ganesh and others},
  journal={arXiv preprint arXiv:1710.03740},
  year={2017}
}

@article{smith2017super,
  title={{Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates}},
  author={Smith, Leslie N and Topin, Nicholay},
  journal={arXiv preprint arXiv:1708.07120},
  year={2017}
}

@inproceedings{reddi2018convergence,
  title={{On the Convergence of Adam and Beyond}},
  author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@misc{mlperf2018,
  title={{MLPerf}},
  howpublished={\url{https://mlperf.org/}},
  year={2018}
}

@article{bahrampour2016comparative,
  title={{Comparative Study of Caffe, Neon, Theano, and Torch for Deep Learning}},
  author={Bahrampour, Soheil and Ramakrishnan, Naveen and Schott, Lukas and Shah, Mohak},
  year={2016}
}

@inproceedings{lim2017enhanced,
  title={{Enhanced Deep Residual Networks for Single Image Super-Resolution}},
  author={Lim, Bee and Son, Sanghyun and Kim, Heewon and Nah, Seungjun and Lee, Kyoung Mu},
  booktitle={The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  volume={1},
  number={2},
  pages={3},
  year={2017}
}

@article{karras2017progressive,
  title={{Progressive Growing of GANs for Improved Quality, Stability, and Variation}},
  author={Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  journal={arXiv preprint arXiv:1710.10196},
  year={2017}
}

@misc{murphy2017deep,
  title={{Deep Learning Benchmarks of NVIDIA Tesla P100 PCIe, Tesla K80, and Tesla M40 GPUs}},
  author={Murphy, J},
  year={2017},
  publisher={Jan}
}

@article{zagoruyko2016wide,
  title={{Wide Residual Networks}},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1605.07146},
  year={2016}
}

@article{williams2009roofline,
  title={{Roofline: An Insightful Visual Performance Model for Multicore Architectures}},
  author={Williams, Samuel and Waterman, Andrew and Patterson, David},
  journal={Communications of the ACM},
  volume={52},
  number={4},
  pages={65--76},
  year={2009},
  publisher={ACM}
}

@article{wah2011caltech,
  title={{The CalTech-UCSD Birds-200-2011 Dataset}},
  author={Wah, Catherine and Branson, Steve and Welinder, Peter and Perona, Pietro and Belongie, Serge},
  year={2011},
  publisher={California Institute of Technology}
}

@article{wang2009learning,
  title={{Learning Models for Object Recognition from Natural Language Descriptions}},
  author={Wang, Josiah and Markert, Katja and Everingham, Mark},
  year={2009}
}

@article{elson2007asirra,
  title={{Asirra: A CAPTCHA that Exploits Interest-aligned Manual Image Categorization}},
  author={Elson, Jeremy and Douceur, John JD and Howell, Jon and Saul, Jared},
  year={2007}
}


@article{masters2018revisiting,
  title={{Revisiting Small Batch Training for Deep Neural Networks}},
  author={Masters, Dominic and Luschi, Carlo},
  journal={arXiv preprint arXiv:1804.07612},
  year={2018}
}

@inproceedings{xie2017aggregated,
  title={Aggregated residual transformations for deep neural networks},
  author={Xie, Saining and Girshick, Ross and Doll{\'a}r, Piotr and Tu, Zhuowen and He, Kaiming},
  booktitle={Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on},
  pages={5987--5995},
  year={2017},
  organization={IEEE}
}

@article{mahajan2018exploring,
  title={Exploring the Limits of Weakly Supervised Pretraining},
  author={Mahajan, Dhruv and Girshick, Ross and Ramanathan, Vignesh and He, Kaiming and Paluri, Manohar and Li, Yixuan and Bharambe, Ashwin and van der Maaten, Laurens},
  journal={arXiv preprint arXiv:1805.00932},
  year={2018}
}

@article{markidis2018nvidia,
  title={NVIDIA Tensor Core Programmability, Performance \& Precision},
  author={Markidis, Stefano and Der Chien, Steven Wei and Laure, Erwin and Peng, Ivy Bo and Vetter, Jeffrey S},
  journal={arXiv preprint arXiv:1803.04014},
  year={2018}
}

@inproceedings{you2018imagenet,
  title={ImageNet training in minutes},
  author={You, Yang and Zhang, Zhao and Hsieh, Cho-Jui and Demmel, James and Keutzer, Kurt},
  booktitle={Proceedings of the 47th International Conference on Parallel Processing},
  pages={1},
  year={2018},
  organization={ACM}
}

@article{akiba2017extremely,
  title={Extremely large minibatch sgd: Training resnet-50 on imagenet in 15 minutes},
  author={Akiba, Takuya and Suzuki, Shuji and Fukuda, Keisuke},
  journal={arXiv preprint arXiv:1711.04325},
  year={2017}
}

@inproceedings{li2014scaling,
  title={Scaling Distributed Machine Learning with the Parameter Server.},
  author={Li, Mu and Andersen, David G and Park, Jun Woo and Smola, Alexander J and Ahmed, Amr and Josifovski, Vanja and Long, James and Shekita, Eugene J and Su, Bor-Yiing},
  booktitle={OSDI},
  volume={14},
  pages={583--598},
  year={2014}
}

@article{jia2018highly,
  title={Highly Scalable Deep Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes},
  author={Jia, Xianyan and Song, Shutao and He, Wei and Wang, Yangzihao and Rong, Haidong and Zhou, Feihu and Xie, Liqiang and Guo, Zhenyu and Yang, Yuanzhou and Yu, Liwei and others},
  journal={arXiv preprint arXiv:1807.11205},
  year={2018}
}

@article{smith2017don,
  title={Don't Decay the Learning Rate, Increase the Batch Size},
  author={Smith, Samuel L and Kindermans, Pieter-Jan and Le, Quoc V},
  journal={arXiv preprint arXiv:1711.00489},
  year={2017}
}

@inproceedings{repo2docker,
	title = {Reproducible Research Environments with Repo2Docker},
	journal = {ICML 2018 Reproducible Machine Learning},
	year = {2018},
	month = {07/2018},
	publisher = {ICML},
	abstract = {Reproducibility challenges in machine learning often center on questions of software engineering practices. Researchers struggle to reproduce another scientist{\textquoteright}s work because they cannot translate a paper into code with similar results or run an author{\textquoteright}s code. repo2docker provides a simple tool for checking the minimum requirements to reproduce a paper by building a Docker image based on a repository path or URL.  Its goal is to minimize the effort needed to convert a static repository into a working software environment. By inspecting a repository for standard configuration files used in contemporary software engineering and leveraging containerization methods, repo2docker deterministically reproduces the environment of the author so the researcher can reproduce the author{\textquoteright}s experiments.},
	keywords = {docker, jupyter, python, reproducibility},
	url = {https://openreview.net/forum?id=B1lYOwuoxm},
	author = {Forde, Jessica and Head, Tim and Holdgraf, Chris and Panda, Yuvi and Nalvarte, Gladys and Pacer, M and Perez, Fernando and Ragan-Kelley, Benjamin and Sundell, Erik}
}

@article{cde,
  author = {Guo, Philip J.},
  title = {{CDE}: A Tool for Creating Portable Experimental Software Packages},
  journal = {Computing in Science and Engineering},
  volume = {14},
  number = {4},
  issn = {1521-9615},
  year = {2012},
  pages = {32-35},
  doi = {http://doi.ieeecomputersociety.org/10.1109/MCSE.2012.36},
  publisher = {IEEE Computer Society},
  address = {Los Alamitos, CA, USA},
}
