
%!TEX root = ../main.tex
 
\section{AI-Native Database}
\label{sec:ANDB}
 
\subsection{Motivation}
\label{sec:motivation}
 

In this section, we explain how AI and DB can benefit from each other. Firstly, we discuss how AI technique can benefit databases (AI4DB). Secondly, we present how database techniques can help AI (DB4AI). Thirdly, we discuss how to fully utilize the new hardware to support both AI and DB. 


\subsubsection{AI4DB}
AI techniques can benefit databases from six aspects.

\hi{Self-configuration.} All databases have hundreds of tuning knobs, which are vital to nearly every aspect of databases, such as performance, availability, robustness, etc. However, these configurations require to be adjusted manually, which not only is time consuming but also cannot find optimal configurations. AI based methods, e.g., deep reinforcement learning, can automatically tune the database knobs. Moreover, other configurations (e.g., software bugs, partition scheme) can also be optimized by AI-based methods. 


\hi{Self-optimizing.} AI can optimize the database performance from many aspects. First, cost/cardinality estimatio is vital to query plan selection. However, database mainly estimates cardinality based on raw statistics (e.g., read/write blocks, backends, deadlocks) and is poor in estimating the resulting row number of each query operator (e.g., hash join, aggregate, filter), with histograms simply. AI-based methods, e.g., Tree-LSTM, can learn data distribution in depth and provide more accurate cardinality estimation. Second, join order selection. Different join schemes have a great impact on query efficiency. And finding the best plan is a NP-hard problem. With static algorithms (e.g., dynamic programming, heuristics algorithm), the performance of join order selection in databse is limited by the effect of the estimator. AI based methods can better choose between different join order plans by taking one-step join as the short-term reward and the execution time as the long-term reward. 

\hi{Self-healing.} High reliability within database has become a critical requirement. But database can crash down for many reasons (e.g., poor performance, hardware/software failovers). So to keep database running is a tough thing. Attempts of traditional database include two aspects. First, set rule-based monitors and alert people when trigger the threshold. But the standards are coarse-grained and cannot effectively prevent accidents. Second, back up data or image periodically. Although that can help in recovery, it cannot assure high reliability. AI-based methods, e,g, CNN, can automatically detect, diagnose, alert and recover from database problems. For example, for distributed database, it supports live data migration when nodes crash or load leans with minimum cost. 

\hi{Self-protecting.} High availability is another important requirement. Database needs to protect the health of each transaction, such as the waiting time and the allocated resources.  Without dynamic protecting mechanisms, database performance can be affected. First, unhealthy resource competition can severely affect database stability. AI-based methods can automatically balance the relation between workspace of a single query and the overall concurrency. Second, anomaly query can cause great loss to database. For example, run-away queries take much longer time than estimated in query plan, which may be hanging. And database will execute such queries for very long time before kill them. With AI techniques, we can not only solve the problems in query processing, but avoid wasting resources for run-away queries.

\hi{Self-inspecting.} Data consistency is vital to data validation. Database maintains data consistency with fixed rules. For example, for single database, data in memory is written back to disk every a number of write operations; for distributed database, data is synchronized between replication nodes using writing logs. With AI techniques, database can automatically learn how to check and assure data consistency and database health. To a higher level, AI models should translate those experiences into publication of instrumentation to help human beings understand.

\hi{Self-assembling}. Each processing layer (e.g., optimizer, executor, storage engine) in a database has several alternatives, each of which has its own advantages. But in traditional database, query is processed in fixed path and cannot take advantage of optional components dynamically. With AI techniques, it can learn how to select proper component in each layer and assemble the complete executing path based on the query and workload characters.


%With AI techniques, the AI model can help in three ways. 1) It can tune automatically. With advanced learning algorithms such as backward propagation, it can tune even better than DBA; 2) It can tune dynamically. Taking environment and workload as input, it can be aware of changes in workload and environment (e.g., hardware migration, various workloads) and tune dynamically.

%\lgl{Self-optimizing.} AI can optimize the database performance from many aspects. First, cost/cardinality estimation ad join order selection are b  database optimizer relies on . 

%Thirdly, database cannot optimize itself dynamically. Optimizer in traditional database cannot assure the performance of the query plan. 1) Database is poor in cardinality estimation, which is vital to query plan selection. Traditionally database mainly collects raw statistics (e.g., read/write blocks, backends, deadlocks) and is poor in estimating the resulting row number of each query operator (e.g., hash join, aggregate, filter) with histograms. With AI techniques, neural network can learn data distribution in depth and provide more accurate cardinality estimation. 2) Database cannot automatically design auxiliary structures (e.g., indexes, materialized views/query tables) to optimize the performance of queries. With AI techniques, the AI model can automatically analyze history workload information and pre-build those structures. For example, DDQN model can learn and predict the features of incoming queries and pre-build indexes on proper columns to accelerate the execution of queries.




%Firstly, data is explosively growing. Since 2010, data has been dramatically increasing, at a rate of doubling every two years. And it is expected to hit 45,000 exabytes in 2020. 
%Explosive data growth has exposed many problems in existing databases. 
%1) Traditional manual tuning methods can no longer meet our needs. Proper configuration is important to database performance. And in cloud, it is impossible for DBAs to manually tune each database instance. So we need to combine AI tech to automate this work.
%2) For distributed database, it has many mechanisms (e.g., data sharding and load balancing) to ensure the performance does not degrade. But those components usually adopt simple strategies, whose error rate is high and can seriously affect the performance. For example, in data sharding, chunks of the same table can have different access frequency. And by evenly distributing data among the nodes some nodes can be overloaded or even crash, while others are idle. So we hope to use AI model to monitor, analyze and maintain load balancing at all times.
%3) The traditional database components such as query optimizer and index building struggle with big data. For example, using simple heuristic algorithm, the optimizer can recommend terrible query plan and waste unaffordable time and resources to execute it. So we hope to use AI algorithm to generate proper query plan within acceptable time.

%Secondly, enterprises have entered the era of integrated data analysis (IDA), in which two or more independent data sets are pooled or combined into one and then statistically analyzed. IDA is costly to conduct.
%1) Different data is stored in different databases. For example, we store highly structured data in relational databases, streaming data in time-series databases and graph data in graph databases. And current database has fixed schemas. So it's expensive to aggregate data from different databases.
%2) Schema is different. Structured data is stored in well-defined schema and is easy to manipulate with SQL.
%Although unstructured data also has internal structure, it is of complex data forms (e.g., e-mails, digital images and navigation details) and is not suitable to be reshaped with pre-defined data models. To force these two kinds of data into the same schemas can lead to great information loss or resource waste.
%emails, text files, web pages, digital images, multimedia content, navigation details and social media posts.


\subsubsection{DB4AI}
AI techniques rely on data heavily. With database storing, managing and manipulating data, the training and learning procedure can be more efficient. But to support AI techniques on database, there are several problems. 
% For any AI technique, it needs large volume of data for model training.

Firstly, AI has different user interfaces to DB. Generally, people write AI models in Python or R languages, but manipulate data from relational database with SQL statements. There are two problems when call AI-related services. 1) People need frequently switch between different systems when writing AI-related applications. 2) It is not easy for traditional data analysts, who only know some SQL knowledge, to write AI code. So if we can extend parsing rules to make SQL support both DB and AI, it would be very convenient to support AI-related  services.

Secondly, DB can optimize AI algorithms. Database can not only provide AI with data, but better support AI services with its mechanisms. We can see from model building training, reusing. 1) With unified SQL interface, user can easily build an AI model using user defined functions or stored procedures. 2) Training AI models consume time with a large number of tensor calculation. Database can better support tensor operators with extended relational algebra and execute them in the executor, which help in model training. 3) Well-trained AI models can be persisted with materialized views or query tables and be reused easily and efficiently.


%they have different data models. For DB, techniques (e.g., CRUD, data integration and data management) are based on relational data model, with which we conduct operations such as join and aggregate. For AI, models (e.g., traditional machine learning, deep learning, reinforcement learning) are based on tensor data model, with tensor operators such as tensor product, addition and division. So the overhead to convert DB data into AI data is relatively high. And we know that, for an AI model, data preprocessing takes most code and wastes much training time. Currently, no practical system can unify these two data models within a single system.

\subsubsection{Heterogeneous Computing Framework}

With Moore's law on the verge of failure, database can no longer rely on single processor to improve processing ability. Now heterogeneous computing framework brings about new potential. Firstly, it incorporates different computing powers (e.g., GPU, NPU, FPGA) and accelerators. Secondly, with dissimilar co-processors handling different tasks, it can gain great improvement in performance and efficiency. But to support heterogeneous computing framework, database needs to solve three main problems.

	$\bullet$ Single resource scheduling in system level. The theoretical architecture and operators in traditional DB are not suitable to computing-intensive chips. That is, simple computing in structured data analysis dose not need chips with high parallel computing power. DB needs to design special heterogeneous computing units, considering of computing and storing resources.

	$\bullet$ Single accelerator architecture. Traditional accelerator architecture only supports OLAP type, For OLTP and HTAP workloads, caused by data consistency between system memory and accelerator's local memory, it is inefficient. DB needs to incorporate new techs, such as CXL released by Intel, to provide efficient memory access methods for accelerators.

	$\bullet$ Single relational algebra. Firstly, to provide heterogeneous computing, we need to define data models for different operating. For example, relational data model is good for data management, but cannot fit TPU/NPU processing model. Besides, We want different computing powers to work together. For example, with extended relational algebra, we can use tensor computing model to accelerate relational operators, such as join and aggregation.


%For example, with the parallel computing design, GPU is good at large-scale data computation, which can efficiently save the training time of neural networks.

