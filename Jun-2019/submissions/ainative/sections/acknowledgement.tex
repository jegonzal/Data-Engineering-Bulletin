%!TEX root = ../main.tex

\section{QUERY Clustering}
\label{sec:cluster}

For workloads including both transactional queries and analytical queries~\cite{DBLP:conf/icde/OhL05}, if the user aims to optimize the latency, we recommend  {\it query-level} tuning; if the user aims to improve the throughput, we recommend  {\it workload-level} tuning. For analytical only queries, we recommend cluster-level tuning to balance the throughput and latency. The key problem in the cluster-level tuning is (1) how to efficiently find the appropriate {\it configuration pattern} for each query and (2) how to cluster the queries based on the configuration pattern.  This section studies these two problems. % in this section.


\subsection{Configuration Pattern}
\label{sec:pattern}
The configuration pattern of a query should include all the knobs used in the DS-DDPG model. Thus a nature idea is to use DS-DDPG to generate a continuous knob configuration and take the knob configuration as the pattern. However it is rather expensive to get the continuous knob values, especially for a large number of queries. More importantly, when we cluster the queries, we do not need to use the accurate configuration pattern; instead approximate patterns are good enough to cluster the queries. 

\lgl{To this end, we discretize the continuous values into discretized values. For example, we can discretize each knob into \{-1,0,+1\}. Specifically, for each knob, if the tuned knob value is around the default value, we set it as 0; 1 if the estimated value is much larger than the default value; -1 if the estimated value is much smaller than the default value.}

\lgl{To avoid the curse of dimensionality when clustering queries, we only choose knobs most frequently tuned by DS-DDPG as the features, about 20 in PostgreSQL.} But the new knob space is still very large. For example, if \lgl{20} knobs are used and each has 3 possible values, then there are $3^{\lgl{20}}$ possible cases. The traditional machine learning methods or regression models are hard  to solve this problem, because they either assume the labels are independent such as Binary Relevance~\cite{DBLP:journals/pai/LuacesDBCB12} or cannot support so many labels such as Classifier chain~\cite{DBLP:journals/ml/ReadPHF11}. 



\vspace{.25em} 
\noindent {\bf  Learning Discrete Configuration Patterns Using Deep Learning.}  We choose the deep learning method to map queries to discrete configuration patterns. As Figure \ref{fig:DL} shows, the \texttt{Vector2Pattern} uses a neural network, which adopts a five-layer architecture. 

The input layer takes the feature vector as input and maps it to the target knob space, in order to make the input and output in the same scale. 

The second layer is designed to explore the configuration patterns. It is a dense layer with \lgl{ReLU} as the activation function %It expends the feature space by 2 times to explore more complex patterns. And  \lgl{ReLU}  
$(y = max(x, 0)$, where $x$ is an input feature and $y$ is the corresponding output features, using to learn two aspects of knowledge: 1) Interaction effects; 2) Non-linear effects. Interaction effects capture the correlations among the input features. For example, feature $v_i$ captures the value difference when other features' value changes. While Non-linear effects learns non-linear mapping relations between input vector and output vector. 

The third layer is a BatchNormal layer. It normalizes the input vector in favor of gaining discretized results. 

The fourth layer has the same function as the second and the value of each feature in this layer's output vector ranges from 0 to 1. But different from \texttt{Predictor} in Section~\ref{sec:tunner}, the last layer uses a {\it sigmoid}  activation function $S(z_{i})$ = $\frac{1}{1+e^{-z_{i}}}$, where $z_i$ is the $i_{th}$ feature of the input vector. It takes a real value as input and outputs a value in 0 to 1. This aims to do a non-linear data transformation and  at the same time keeps the features in the limited range.  

%\lgl{It is nonlinear, continuously differentiable and it changes monotonously.}  % To avoid sparing space causes too many $"$noise$"$ points, we also convert the knobs of numerical type into discrete values. 

For the DL model, we also append a step function to the network's end and use the output layer as a probability distribution function: for each feature $y$ in the output vector, the resulting bit is -1 if $y$ is below 0.5; 0 if $y$ equals to 0.5; and 1 otherwise. In this way, the DL model can automatically finish data transformation and discretization work. 

%\lgl{what is y?}\lgl{give more details of deep learning model!!!}


\begin{figure}[!t]\centering
\vspace{-.5em}
\includegraphics[width=.45\textwidth, height=.27\textwidth]{tuning_figs/DL_x.eps}
\vspace{-2em}
\caption{Architecture of the DL model}
\label{fig:DL}
\vspace{-2.5em}
\end{figure} 


\vspace{.25em} 
\noindent{\bf Workflow of \texttt{Vector2Pattern}.} 
The DL model works in 4 steps: 1) For each training sample $\langle q, p_r\rangle$, where $q$ is a query and $p_r$ is the real pattern that matches $q$,  compute the feature vector $v$ of query $q$; 2)  Propagate these features through its network; 3) Output an estimated  pattern $p_e$; 4) Based on the output pattern $p_e$ and the actual pattern $p_r$, update the weights in the network by minimizing $|p_e-p_r|$. 



\vspace{.25em} 
\noindent{\bf Training Step. } We need to generate a large volume of samples to train \texttt{Vector2Pattern} until the performance on a new testing set is good enough (i.e., high generalization ability). Each training sample is in the form of $\langle q, p\rangle$, where $q$ is a query statement  and $p$ is a configuration pattern under which the database can efficiently execute $q$. To collect these samples, we follow 3 steps: 1) Train the DS-DDPG model until it converges; 2) Select 10,000 real queries from the training data; 3) For each query $q$ in the selected queries, use \texttt{Query2Vector} to featurize $q$ and get $v$. We input the vector $v$ into the DS-DDPG model, and get a recommended configuration to measure the performance of this query. If the performance is good enough, we discretize this configuration into  pattern  %$p$. $p$ is adopted as the pattern of $q$ 
and the iteration terminates. 

 %When the sample set is large enough, we split it into training set and testing set by 8:2. On the training set, we train the DL model using \texttt{adam}, which has been explained in Section~\ref{sec:tunner_training}.

\vspace{-.25em}
\subsection{Query Clustering}
\vspace{-.25em}


After gaining the suitable configuration pattern for each query, we classify the queries into different clusters based on the similarity of these patterns. Any clustering algorithms can be used to cluster the configurations, and we take {\tt DBSCAN}~\cite{DBLP:conf/kdd/EsterKSX96} as an example. Based on the configuration pattern,  {\tt DBSCAN} groups the patterns together that are close to each other according to a distance measurement and the minimum number of points to be clustered together. 


